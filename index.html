<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" >
  <meta name="description" content="Ph.D. student at Yale">
  <title>Xiangru (Robert) Tang - Yale</title>
  <link rel="shortcut icon" href="images/tree-12c-194370/tree-12c-152-194370.png" />
  <link href='assets/css_s/bootstrap.min.css' rel='stylesheet'>
  <link href='assets/css_s/main_style.css' rel='stylesheet'>
  <link href='https://fonts.googleapis.com/css?family=Nunito:200,300,400,500,600,700,800,900,100italic,100,300italic,400italic,500italic,900italic,700italic' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Lato:200,300,400,500,600,700,800,900,100italic,100,300italic,400italic,500italic,900italic,700italic' rel='stylesheet' type='text/css'>
<script src="http://cdn.static.runoob.com/libs/jquery/2.1.1/jquery.min.js"></script>
<script src="http://cdn.static.runoob.com/libs/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<link href="bootstrap.min.css" rel="stylesheet" media="screen" />
<link href="font-awesome-4.7.0/css/font-awesome.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
<LINK rel=stylesheet type=text/css  href="./jemdoc.css"><LINK rel="shortcut icon" href="rui.ico">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<script async defer src="buttons.js"></script>
<script src="jquery.scrollUp.js"></script>
<!-- Place this tag in your head or just before your close body tag. -->
<script async defer src="https://buttons.github.io/buttons.js"></script>
	
<SCRIPT type=text/javascript>
  
var a = document.querySelector(".navbar-toggle");
    $(".navbar-nav li a").on("click",function () {
        a.click();
    });

	
// Toggle Display of BibTeX
function toggleBibtex(articleid) {
  var bib = document.getElementById(articleid);
  // Toggle 
    if(bib.style.display == "none") {
      var x = document.getElementsByClassName("blockcontent");
      var i;
      for (i = 0; i < x.length; i++) {
          x[i].style.display = "none";
      }
      bib.style.display = "";
    }
    else {
      bib.style.display = "none";
    }

}

function showall(){  
    var items = document.getElementsByClassName('coauthor');
    if(document.getElementById("list_type").text == "Full list"){
        document.getElementById("list_type").text = "Selected";
        for (i = 0; i < items.length; ++i)
            {
                items[i].style.display = 'block';
            }
    }else{
        document.getElementById("list_type").text = "Full list";
        for (i = 0; i < items.length; ++i)
            {
                items[i].style.display = 'none';
            }
    }
    
}
</SCRIPT>

<script>
$(function () {
    $.scrollUp({
        animation: 'fade',
        scrollImg: {
            active: true,
            type: 'background',
            src: 'imgs/top.png'
        }
    });
});
$('#scrollUpTheme').attr('href', 'image.css?1.1');
$('.image-switch').addClass('active');

</script>
	
	

    <!-- Bootstrap & MDB
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />
 -->
    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>">
    
    <!-- <link rel="stylesheet" href="/assets/css/main.css"> -->
    <link rel="stylesheet" href="assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/">

    <!-- <script src="/assets/js/theme.js"></script> -->
    <script src="assets/js/theme.js"></script>
    <!-- <script src="/assets/js/dark_mode.js"></script> -->
    <script src="assets/js/dark_mode.js"></script>

	
	
	
	
	
	
</head>
<body>
  <div id='header' class ='bg'>
    <div id='header-inner'>
      <div id="author-avatar"></div>
      <div class='header-text'>
        <div class='header-text-name'>
            Robert Tang <span style="font-family:STFangsong; font-size:24pt"> </span>
        </div>
        <div class='header-text-email'>
		<p></p>
          <p>Ph.D. Candidate</p>
          <p>Computer Science, Yale University, U.S.</p>
          <p>xiangru.tang@yale.edu</p>
		<p></p>
        </div>

        <div class='header-text-items'>
          <ul class="icons">
            <li><a href="https://scholar.google.com/citations?user=gGcRkpYAAAAJ" class="icon ai-google-scholar"><span class="label"></span></a></li>
	    <li><a href="https://engineering.yale.edu/academic-study/departments/computer-science/people#phdstudents" class="icon fa-home"><span class="label"></span></a></li>
            <li><a href="https://twitter.com/intent/follow?original_referer=https%3A%2F%2Fctrlnlg.github.io%2F&region=follow_link&screen_name=XiangruTang" class="icon fa-twitter"><span class="label"></span></a></li>
            <li><a href="https://github.com/tangxiangru" class="icon fa-github"><span class="label"></span></a></li>
           <!--  <li><a href="files/.pdf" class="icon fa-file-pdf-o"><span class="label"></span></a></li> -->
          </ul>

	       <div class='header-text-email'>
		<p></p>
		<p></p>
        </div>
		
        </div>
      </div>
    </div>
  </div>

  <div class='container'>
    <div class='col-xs-1'>
    </div>
    <div class='col-xs-10'>
      <div class='row'>
	      

<p>I'm a final-year Ph.D. candidate in Computer Science at <a href="http://www.yale.edu/" target="_blank" rel="noopener noreferrer">Yale University</a>, where I am very fortunately advised by <a href="https://scholar.google.com/citations?user=YvjuUugAAAAJ" target="_blank" rel="noopener noreferrer">Mark Gerstein</a>. I work closely with <a href="https://armancohan.com/" target="_blank" rel="noopener noreferrer">Arman Cohan</a> as my “advisor of record”. I also work with <a href="https://engineering.yale.edu/research-and-faculty/faculty-directory/smita-krishnaswamy" target="_blank" rel="noopener noreferrer">Smita Krishnaswamy</a>. Previously, I got my master's from <a href="https://cpsc.yale.edu/" target="_blank" rel="noopener noreferrer">Yale CS</a> as well, advised by (the late) <a href="https://www.cs.yale.edu/homes/radev/" target="_blank" rel="noopener noreferrer">Dragomir Radev</a>.
I have been a graduate affiliate at Grace Hopper College since 2021. 
</p>
		
<p>	
I am actively pursuing the development of <font color="#A6192E">"biomedical superintelligence"</font> and building AI agents to automate biomedical research, e.g.,

</p>
	      
<ul>

 <li>
<strong>Reasoning-Intensive Scientific Discovery:</strong> AI scientists capable of verifiable reasoning can autonomously design, plan, and perform experiments by code execution [ChemAgent, MedAgents, Struc-Bench, OctoPack, OpenHands].
	 
    </li>
    <li>
<strong>Tool Use & Repository-Level Code Generation:</strong> AI scientists could integrate AI models and specialized tools with experimental platforms [Risks of AI Scientist, BioCoder, ToolLLM, ML-Bench, Agents, Data Interpreter]. 
	 
    </li>
    <li>
<strong>Open-Ended Drug Design:</strong> AI scientists can impact areas ranging from molecule modeling,  protein folding, and virtual cell simulation to developing new therapies [DHR, MolLM, GeneBERT].

    </li>


   
</ul>





<!--I am looking for grads / undergrads to collaborate and actively engage in mentorship. Feel free to 
<link href="https://assets.calendly.com/assets/external/widget.css" rel="stylesheet">
<script src="https://assets.calendly.com/assets/external/widget.js" type="text/javascript" async></script>
<a href="" onclick="Calendly.initPopupWidget({url: 'https://calendly.com/xiangru-tang/zoom?background_color=fae7e7&primary_color=2f62aa'});return false;"> contact me</a> if you are starting in the field.   
-->


	       <br>
My research is supported by Schmidt Futures.

 <br>
	       <br>


I am currently a research intern with Google Search and Google DeepMind. I have interned at Microsoft and Tencent before.<!-- <font color="#A6192E"> I am actively seeking academic or industry research positions starting in Fall 2026, with a focus on agentic AI for biomedical discovery ("AI Scientists"). 
 </font> Please feel free to get in touch regarding relevant opportunities.-->
     


	      
 <br>
 <br>



	      	      
<div class="post">
  <article>
    <div class="cv">
          <h2 id="Honors"><font size="5" color="#0f4d92"><b>Honors</b></font></h2>
          <div>
		  
 
  
 <strong>IJCAI 2024 AI for Research Workshop Best Paper Award</strong>
<br>
  <strong>ICML 2025 CFAgentic Workshop Best Paper Runner-Up Award</strong>


		   <br>
 <br>

           </div>
      </div>
   </article>
  </div>	  	  


	      

<!-- <a href="https://forms.gle/pxSzza4VgJrxmVfy6" target="_blank" rel="noopener noreferrer">drop me a line</a> -->

  <br>

	     
	   <br>
	      
	      <div class="post">
  <article>
    <div class="cv">
          <h2 id="papers"><font size="5" color="#0f4d92"><b>Recent Preprints</b></font></h2>
          <div>

	      Discover the  <a href="https://scholar.google.com/citations?hl=en&user=gGcRkpYAAAAJ&view_op=list_works&sortby=pubdate" target="_blank"><img src="./images/google_scholar.png" height="20" alt="google scholar"></a> | 
<a href="https://www.semanticscholar.org/author/Xiangru-Tang/47274259?sort=pub-date" target="_blank"><img src="./images/semantic_scholar_logo.png" height="20" alt="semantic scholar"></a> 

		  	        
<UL>
    <LI>
    <B style="color: #224b8d">[3] Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving
</B><BR>
<B>Xiangru Tang</B><span style="color: gray;">, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei, Peng Xia, Fang Wu, He Zhu, Ge Zhang, Jiaheng Liu, Xingyao Wang, Sirui Hong, Chenglin Wu, Hao Cheng, Chi Wang, Wangchunshu Zhou.</span>
<BR>
	  <B>  ICML 2025 Workshop on CFAgentic (Best Paper Runner-up Award) </B>  <BR>
      [<a href="2507.06229">PDF</a>]
      [<a href="javascript:toggleBibtex('AgentKB_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('AgentKB_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/OPPO-PersonalAI/Agent-KB" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star OPPO-PersonalAI/Agent-KB on GitHub">Agent-KB</a>	   
	
	<div id=AgentKB_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Current AI agents cannot effectively learn from each other's problem-solving experiences or use past successes to guide self-reflection and error correction in new tasks. We introduce Agent KB, a shared knowledge base that captures both high-level problem-solving strategies and detailed execution lessons, enabling knowledge transfer across agent frameworks. Agent KB implements a novel teacher-student dual-phase retrieval mechanism where student agents retrieve workflow-level patterns for strategic guidance while teacher agents identify execution-level patterns for refinement. This hierarchical approach enables agents to break out of limited reasoning pathways by incorporating diverse strategies from external sources. Evaluations on the GAIA benchmark demonstrate substantial performance gains, with Agent KB improving success rates by up to 6.06 percentage points overall under pass@1. For SWE-bench code repair tasks, our system significantly improved resolution rates, with o3-mini achieving an 8.67 percentage point gain (23 percent to 31.67 percent) in pass@1. Our ablation studies demonstrate that the refinement module proves most critical, with its removal causing a 3.85% drop on challenging Level 3 tasks, highlighting that effective knowledge transfer necessitates both strategic guidance and execution-level refinement.
			    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=AgentKB_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{tang2025agent,
  title={Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving},
  author={Tang, Xiangru and Qin, Tianrui and Peng, Tianhao and Zhou, Ziyang and Shao, Daniel and Du, Tingting and Wei, Xinming and Xia, Peng and Wu, Fang and Zhu, He and others},
  journal={arXiv preprint arXiv:2507.06229},
  year={2025}
}</pre>
       </div>
    </LI>
</UL>




		  
		  
	        
<UL>
    <LI>
    <B style="color: #224b8d">[2] MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for Complex Medical Reasoning
</B><BR>
<B>Xiangru Tang</B><span style="color: gray;">, Daniel Shao, Jiwoong Sohn, Jiapeng Chen, Jiayi Zhang, Jinyu Xiang, Fang Wu, Yilun Zhao, Chenglin Wu, Wenqi Shi, Arman Cohan, Mark Gerstein.</span>
<BR>
   <B> arXiv preprint arXiv:2503.07459, Patterns (in review)</B><BR>
	<font color="#A6192E">"Thinking models (DeepSeek R1 and OpenAI o3) show exceptional performance on medical QA tasks."</font> <a href="https://x.com/XiangruTang/status/1902885945596137484" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:20px;"></i></a><br>
      [<a href="https://arxiv.org/abs/2503.07459">PDF</a>]
      [<a href="javascript:toggleBibtex('MedAgentsBench_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('MedAgentsBench_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/gersteinlab/medagents-benchmark" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star gersteinlab/medagents-benchmark on GitHub">MedagentsBench</a>	   
	
	<div id=MedAgentsBench_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Large Language Models (LLMs) have shown impressive performance on existing medical question-answering benchmarks. This high performance makes it increasingly difficult to meaningfully evaluate and differentiate advanced methods. We present MedAgentsBench, a benchmark that focuses on challenging medical questions requiring multi-step clinical reasoning, diagnosis formulation, and treatment planning-scenarios where current models still struggle despite their strong performance on standard tests. Drawing from seven established medical datasets, our benchmark addresses three key limitations in existing evaluations: (1) the prevalence of straightforward questions where even base models achieve high performance, (2) inconsistent sampling and evaluation protocols across studies, and (3) lack of systematic analysis of the interplay between performance, cost, and inference time. Through experiments with various base models and reasoning methods, we demonstrate that the latest thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in complex medical reasoning tasks. Additionally, advanced search-based agent methods offer promising performance-to-cost ratios compared to traditional approaches. Our analysis reveals substantial performance gaps between model families on complex questions and identifies optimal model selections for different computational constraints. Our benchmark and evaluation framework are publicly available at https://github.com/gersteinlab/medagents-benchmark.
			    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=MedAgentsBench_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{tang2025medagentsbench,
  title={MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for Complex Medical Reasoning},
  author={Tang, Xiangru and Shao, Daniel and Sohn, Jiwoong and Chen, Jiapeng and Zhang, Jiayi and Xiang, Jinyu and Wu, Fang and Zhao, Yilun and Wu, Chenglin and Shi, Wenqi and others},
  journal={arXiv preprint arXiv:2503.07459},
  year={2025}
} </pre>
       </div>
    </LI>
</UL>

	  

		  

<UL>
    <LI>
    <B style="color: #224b8d">[1] BC-Design: A Biochemistry-Aware Framework for High-Precision Inverse Protein Folding
</B><BR>
<B>Xiangru Tang*</B><span style="color: gray;">, Xinwu Ye*, Fang Wu*, Yanjun Shao, Yin Fang, Siming Chen, Dong Xu, Mark Gerstein.</span>
<BR> 
   <B>bioRxiv 2024, Nature (in review)</B><BR>

	<font color="#A6192E">"A quantum leap in inverse protein folding from 67% to 88%!"</font> <a href="https://x.com/XiangruTang/status/1854268118916542644" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:20px;"></i></a><br>
      [<a href="https://www.biorxiv.org/content/10.1101/2024.10.28.620755v1">PDF</a>]
      [<a href="javascript:toggleBibtex('ipf_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('ipf_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/gersteinlab/BC-Design" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star gersteinlab/BC-Design on GitHub">BC-Design</a>	   
	
	<div id=ipf_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Inverse protein folding, which aims to design amino acid sequences for desired protein structures, is fundamental to protein engineering and therapeutic development. While recent deep-learning approaches have made remarkable progress in addressing this challenge, they typically represent biochemical properties as discrete features associated with individual residues. Here, we present BC-Design, an approach that explicitly represents these properties as decorations on randomly sampled points on exterior surfaces and within internally bound regions representing the complete molecular extent of the protein. This provides a more natural way to capture the spatial distribution of properties. We demonstrate that BC-Design significantly outperforms all current methods, improving sequence recovery from 67% to 88.37% over the state-of-the-art methods (a 21.32% absolute improvement) and reducing perplexity from 2.4 to 1.47 (a 39.51% relative improvement) on the CATH 4.2 benchmark. Notably, our model exhibits robust generalization across diverse protein characteristics, achieving consistently high performance on proteins of varying sizes (50-500 residues), structural complexity (measured by contact order), and all major CATH fold classes. Through ablation tests, we compare the relative contribution of both structure encoding information and the encoded property information, and we show that both substantially contribute equally to this strong performance. Overall, this opens new avenues for computational protein engineering and drug discovery.
			    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=ipf_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{tang2024bc,
  title={BC-Design: A Biochemistry-Aware Framework for High-Precision Inverse Protein Folding},
  author={Tang, Xiangru and Ye, Xinwu and Wu, Fang and Shao, Yanjun and Fang, Yin and Chen, Siming and Xu, Dong and Gerstein, Mark},
  journal={bioRxiv},
  pages={2024--10},
  year={2024},
  publisher={Cold Spring Harbor Laboratory}
} </pre>
       </div>
    </LI>
</UL>





		  
 <br>

           </div>
      </div>
   </article>
  </div>	  	  


	      






		  
<div class="post">
  <article>
    <div class="cv">
          <h2 id="papers"><font size="5" color="#0f4d92"><b>Selected Publications</b></font></h2>
          <div>
	

		  
		

		  

		  
 <UL>
    <LI>
    <B style="color: #224b8d">[18] MMSciBench: Benchmarking Language Models on Multimodal Scientific Problems</B><BR>
	   <span style="color: gray;">Xinwu Ye, Chengfan Li, Siming Chen, Wei Wei,</span> <B>Xiangru Tang</B>
<BR>
      <B> ACL 2025 Findings</B> <BR>
      [<a href="https://arxiv.org/abs/2503.01891">PDF</a>]
      [<a href="javascript:toggleBibtex('MMSciBench_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('MMSciBench_bib')" target=_self>Bib</a>]
	    <div style="padding-top:5px"> <a class="github-button" href="https://github.com/xinwuye/MMSciBench-code" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star xinwuye/MMSciBench-code on GitHub">MMSciBench</a>	   

	<div id=MMSciBench_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Recent advances in large language models (LLMs) and vision-language models (LVLMs) have shown promise across many tasks, yet their scientific reasoning capabilities remain untested, particularly in multimodal settings. We present MMSciBench, a benchmark for evaluating mathematical and physical reasoning through text-only and text-image formats, with human-annotated difficulty levels, solutions with detailed explanations, and taxonomic mappings. Evaluation of state-of-the-art models reveals significant limitations, with even the best model achieving only \textbf{63.77\%} accuracy and particularly struggling with visual reasoning tasks. Our analysis exposes critical gaps in complex reasoning and visual-textual integration, establishing MMSciBench as a rigorous standard for measuring progress in multimodal scientific understanding. The code for MMSciBench is open-sourced at GitHub, and the dataset is available at Hugging Face.
			    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=MMSciBench_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{ye2025mmscibench,
  title={MMSciBench: Benchmarking Language Models on Multimodal Scientific Problems},
  author={Ye, Xinwu and Li, Chengfan and Chen, Siming and Tang, Xiangru and Wei, Wei},
  journal={arXiv preprint arXiv:2503.01891},
  year={2025}
} </pre>
       </div>
    </LI>
</UL>



 <UL>
    <LI>
    <B style="color: #224b8d">[17] LocAgent: Graph-Guided LLM Agents for Code Localization</B><BR>
	   <span style="color: gray;">Zhaoling Chen*, </span><B>Xiangru Tang*</B><span style="color: gray;">, Gangda Deng*, Fang Wu, Jialong Wu, Zhiwei Jiang, Viktor Prasanna, Arman Cohan, Xingyao Wang.</span>
<BR>
      <B> ACL 2025 </B> <BR>
	    <font color="#A6192E">"No need to embed the entire repo, agent + graph-based indexing is all you need!"</font> <a href="https://x.com/XiangruTang/status/1900392655009333338" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:20px;"></i></a><br>

      [<a href="https://arxiv.org/abs/2503.09089">PDF</a>]
      [<a href="javascript:toggleBibtex('LocAgent_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('LocAgent_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/gersteinlab/LocAgent" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star gersteinlab/LocAgent on GitHub">LocAgent</a>	   

	<div id=LocAgent_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Code localization--identifying precisely where in a codebase changes need to be made--is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code sections. The challenge lies in bridging natural language problem descriptions with the appropriate code elements, often requiring reasoning across hierarchical structures and multiple dependencies. We introduce LocAgent, a framework that addresses code localization through graph-based representation. By parsing codebases into directed heterogeneous graphs, LocAgent creates a lightweight representation that captures code structures (files, classes, functions) and their dependencies (imports, invocations, inheritance), enabling LLM agents to effectively search and locate relevant entities through powerful multi-hop reasoning. Experimental results on real-world benchmarks demonstrate that our approach significantly enhances accuracy in code localization. Notably, our method with the fine-tuned Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA proprietary models at greatly reduced cost (approximately 86% reduction), reaching up to 92.7% accuracy on file-level localization while improving downstream GitHub issue resolution success rates by 12% for multiple attempts (Pass@10).                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=LocAgent_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{chen2025locagent,
  title={LocAgent: Graph-Guided LLM Agents for Code Localization},
  author={Chen, Zhaoling and Tang, Xiangru and Deng, Gangda and Wu, Fang and Wu, Jialong and Jiang, Zhiwei and Prasanna, Viktor and Cohan, Arman and Wang, Xingyao},
  journal={arXiv preprint arXiv:2503.09089},
  year={2025}
} </pre>
       </div>
    </LI>
</UL>


		  

<UL>
    <LI>
    <B style="color: #224b8d">[16] ML-Bench: Evaluating Large Language Models and Agents for Machine Learning Tasks on Repository-Level Code
</B><BR>
<B>Xiangru Tang*</B><span style="color: gray;">, Yuliang Liu*, Zefan Cai*, Junjie Lu, Yichi Zhang, Yanjun Shao, Zexuan Deng, Helan Hu, Kaikai An, Ruijun Huang, Shuzheng Si, Sheng Chen, Haozhe Zhao, Liang Chen, Yan Wang, Tianyu Liu, Zhiwei Jiang, Baobao Chang, Yujia Qin, Wangchunshu Zhou, Yilun Zhao, Arman Cohan, Mark Gerstein.</span>
<BR>
	     <B>Nature Computational Science (in review)</B> <br>
	    
	 <B>ICLR 2025 Deep Learning for Code</B> <br>

   <B>ICLR 2025 Agentic AI for Scientific Discovery</B><BR>
	<font color="#A6192E">"Can LLMs do machine learning tasks?"</font> <a href="https://x.com/XiangruTang/status/1801088490219676120" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:20px;"></i></a><br>
      [<a href="https://arxiv.org/abs/2311.09835">PDF</a>]
      [<a href="javascript:toggleBibtex('ML-Bench_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('ML-Bench_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/gersteinlab/ML-Bench" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star gersteinlab/ML-Bench on GitHub">ML-Bench</a>	   
	
	<div id=ML-Bench_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Despite Large Language Models (LLMs) like GPT-4 achieving impressive results in function-level code generation, they struggle with repository-scale code understanding (e.g., coming up with the right arguments for calling routines), requiring a deeper comprehension of complex file interactions. Also, recently, people have developed LLM agents that attempt to interact with repository code (e.g., compiling and evaluating its execution), prompting the need to evaluate their performance. These gaps have motivated our development of ML-Bench, a benchmark rooted in real-world programming applications that leverage existing code repositories to perform tasks. Addressing the need for LLMs to interpret long code contexts and translate instructions into precise, executable scripts, ML-Bench encompasses annotated 9,641 examples across 18 GitHub repositories, challenging LLMs to accommodate user-specified arguments and documentation intricacies effectively. To evaluate both LLMs and AI agents, two setups are employed: ML-LLM-Bench for assessing LLMs' text-to-code conversion within a predefined deployment environment, and ML-Agent-Bench for testing autonomous agents in an end-to-end task execution within a Linux sandbox environment. Our findings indicate that while GPT-4o leads with a Pass@5 rate surpassing 50%, there remains significant scope for improvement, highlighted by issues such as hallucinated outputs and difficulties with bash script generation. Notably, in the more demanding ML-Agent-Bench, GPT-4o achieves a 76.47% success rate, reflecting the efficacy of iterative action and feedback in complex task resolution.
			    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=ML-Bench_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{tang2023ml,
  title={ML-Bench: Evaluating Large Language Models and Agents for Machine Learning Tasks on Repository-Level Code},
  author={Tang, Xiangru and Liu, Yuliang and Cai, Zefan and Shao, Yanjun and Lu, Junjie and Zhang, Yichi and Deng, Zexuan and Hu, Helan and Yang, Zengxian and An, Kaikai and others},
  journal={arXiv preprint arXiv:2311.09835},
  year={2023}
} </pre>
       </div>
    </LI>
</UL>






		  
		

		    <UL>
    <LI>
    <B style="color: #224b8d">[15] Risks of AI Scientists: Prioritizing Safeguarding Over Autonomy</B><BR>
	   <B>Xiangru Tang</B><span style="color: gray;">, Qiao Jin, Kunlun Zhu, Tongxin Yuan, Yichi Zhang, Wangchunshu Zhou, Meng Qu, Yilun Zhao, Jian Tang, Zhuosheng Zhang, Arman Cohan, Zhiyong Lu, Mark Gerstein.</span>
<BR>
            <B> Nature Communications, 2025 </B> (IF 14.7) <br>
            <B> ICLR 2024 Workshop on LLM Agents </B> 

	    <BR>
      [<a href="https://arxiv.org/abs/2402.04247">PDF</a>]
      [<a href="javascript:toggleBibtex('sciagent_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('sciagent_bib')" target=_self>Bib</a>]

	<div id=sciagent_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provide a scoping review of the limited existing works. Based on our analysis, we propose a triadic framework involving human regulation, agent alignment, and an understanding of environmental feedback (agent regulation) to mitigate these identified risks. Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents and advocate for the development of improved models, robust benchmarks, and comprehensive regulations to address these issues effectively.
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=sciagent_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{tang2024prioritizing,
  title={Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science},
  author={Tang, Xiangru and Jin, Qiao and Zhu, Kunlun and Yuan, Tongxin and Zhang, Yichi and Zhou, Wangchunshu and Qu, Meng and Zhao, Yilun and Tang, Jian and Zhang, Zhuosheng and others},
  journal={arXiv preprint arXiv:2402.04247},
  year={2024}
}  </pre>
       </div>
    </LI>
</UL>





		    <UL>
    <LI>
    <B style="color: #224b8d">[14] ChemAgent: Self-updating Memories in Large Language Models Improves Chemical Reasoning</B><BR>
	   <B>Xiangru Tang*</B><span style="color: gray;">, Tianyu Hu*, Muyang Ye*, Yanjun Shao*, Xunjian Yin, Siru Ouyang, Wangchunshu Zhou, Pan Lu, Zhuosheng Zhang, Yilun Zhao, Arman Cohan, Mark Gerstein.</span>
<BR>
      <B> ICLR 2025 </B> <BR>
	    <font color="#A6192E">"Enable LLMs to continuously improve through experience."</font> <a href="https://x.com/XiangruTang/status/1897407289025318968" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:20px;"></i></a><br>

      [<a href="https://arxiv.org/abs/2501.06590">PDF</a>]
      [<a href="javascript:toggleBibtex('ChemAgent_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('ChemAgent_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/gersteinlab/chemagent" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star gersteinlab/chemagent on GitHub">ChemAgent</a>	   

	<div id=ChemAgent_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Chemical reasoning usually involves complex, multi-step processes that demand precise calculations, where even minor errors can lead to cascading failures. Furthermore, large language models (LLMs) encounter difficulties handling domain-specific formulas, executing reasoning steps accurately, and integrating code effectively when tackling chemical reasoning tasks. To address these challenges, we present ChemAgent, a novel framework designed to improve the performance of LLMs through a dynamic, self-updating library. This library is developed by decomposing chemical tasks into sub-tasks and compiling these sub-tasks into a structured collection that can be referenced for future queries. Then, when presented with a new problem, ChemAgent retrieves and refines pertinent information from the library, which we call memory, facilitating effective task decomposition and the generation of solutions. Our method designs three types of memory and a library-enhanced reasoning component, enabling LLMs to improve over time through experience. Experimental results on four chemical reasoning datasets from SciBench demonstrate that ChemAgent achieves performance gains of up to 46% (GPT-4), significantly outperforming existing methods. Our findings suggest substantial potential for future applications, including tasks such as drug discovery and materials science. 
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=ChemAgent_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{tang2025chemagent,
  title={ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning},
  author={Tang, Xiangru and Hu, Tianyu and Ye, Muyang and Shao, Yanjun and Yin, Xunjian and Ouyang, Siru and Zhou, Wangchunshu and Lu, Pan and Zhang, Zhuosheng and Zhao, Yilun and others},
  booktitle={The Thirteenth International Conference on Learning Representations}
} </pre>
       </div>
    </LI>
</UL>




		  

		  <UL>
    <LI>
    <B style="color: #224b8d">[13] Fast, Sensitive Detection of Protein Homologs Using Deep Dense Retrieval</B><BR>

	    <span style="color: gray;">Liang Hong*, Zhihang Hu*, Siqi Sun*,</span> <B>Xiangru Tang*</B><span style="color: gray;">, Jiuming Wang, Qingxiong Tan, Liangzhen Zheng, Sheng Wang, Sheng Xu, Irwin King, Mark Gerstein, Yu Li.</span><BR>

      <B> Nature Biotechnology, 2024</B> (IF 33.1) <BR>
<font color="#A6192E">"Up to 28,700 times faster than HMMER!"</font> <a href="https://x.com/LeoTZ03/status/1822141114410729660" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:20px;"></i></a><br>

      [<a href="https://www.nature.com/articles/s41587-024-02353-6">PDF</a>]
      [<a href="javascript:toggleBibtex('dhr_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('dhr_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/ml4bio/Dense-Homolog-Retrieval" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star ml4bio/Dense-Homolog-Retrieval on GitHub">DPR</a>	   

	<div id=dhr_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
The identification of protein homologs in large databases using conventional methods, such as protein sequence comparison, often misses remote homologs. Here, we offer an ultrafast, highly sensitive method, dense homolog retriever (DHR), for detecting homologs on the basis of a protein language model and dense retrieval techniques. Its dual-encoder architecture generates different embeddings for the same protein sequence and easily locates homologs by comparing these representations. Its alignment-free nature improves speed and the protein language model incorporates rich evolutionary and structural information within DHR embeddings. DHR achieves a >10% increase in sensitivity compared to previous methods and a >56% increase in sensitivity at the superfamily level for samples that are challenging to identify using alignment-based approaches. It is up to 22 times faster than traditional methods such as PSI-BLAST and DIAMOND and up to 28,700 times faster than HMMER. The new remote homologs exclusively found by DHR are useful for revealing connections between well-characterized proteins and improving our knowledge of protein evolution, structure and function.
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=dhr_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{hong2024fast,
  title={Fast, sensitive detection of protein homologs using deep dense retrieval},
  author={Hong, Liang and Hu, Zhihang and Sun, Siqi and Tang, Xiangru and Wang, Jiuming and Tan, Qingxiong and Zheng, Liangzhen and Wang, Sheng and Xu, Sheng and King, Irwin and others},
  journal={Nature Biotechnology},
  pages={1--13},
  year={2024},
  publisher={Nature Publishing Group US New York}
} </pre>
       </div>
    </LI>
</UL>









		  

		  <UL>
    <LI>
    <B style="color: #224b8d">[12] MIMIR: A Customizable Agent Tuning Platform for Enhanced Scientific Applications</B><BR>
	   <B>Xiangru Tang*</B><span style="color: gray;">, Chunyuan Deng*, Hanmin Wang*, Haoran Wang*, Yilun Zhao, Wenqi Shi, Yi Fung, Wangchunshu Zhou, Jiannan Cao, Heng Ji, Arman Cohan, Mark Gerstein.</span>
<BR>
      <B> EMNLP 2024 </B> <BR>
      [<a href="https://arxiv.org/abs/2404.04285">PDF</a>]
      [<a href="javascript:toggleBibtex('mimir_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('mimir_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/gersteinlab/MIMIR" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star gersteinlab/MIMIR on GitHub">MIMIR</a>	   

	<div id=mimir_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Recently, large language models (LLMs) have evolved into interactive agents, proficient in planning, tool use, and task execution across various tasks. However, without agent-tuning, open-source models like LLaMA2 currently struggle to match the efficiency of larger models such as GPT-4 in scientific applications due to a lack of agent-tuning datasets. In response, we introduce MIMIR, a streamlined platform offering a customizable pipeline that enables users to leverage both private knowledge and publicly available, legally compliant datasets at scale for agent tuning. Additionally, MIMIR supports the generation of general instruction-tuning datasets from the same input. This dual capability ensures LLM agents developed through the platform possess specific agent abilities and general competencies. MIMIR integrates these features into an end-to-end platform, facilitating everything from the uploading of scientific data to one-click agent fine-tuning. MIMIR is publicly released and actively maintained at https://github.com/gersteinlab/MIMIR, along with a demo video1 for a quick start, calling for broader development.
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=mimir_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{tang-etal-2024-mimir,
    title = "{MIMIR}: A Customizable Agent Tuning Platform for Enhanced Scientific Applications",
    author = "Tang, Xiangru  and
      Deng, Chunyuan  and
      Wang, Hanmin  and
      Wang, Haoran  and
      Zhao, Yilun  and
      Shi, Wenqi  and
      Fung, Yi  and
      Zhou, Wangchunshu  and
      Cao, Jiannan  and
      Ji, Heng  and
      Cohan, Arman  and
      Gerstein, Mark",
    editor = "Hernandez Farias, Delia Irazu  and
      Hope, Tom  and
      Li, Manling",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-demo.49",
    pages = "486--496",
} </pre>
       </div>
    </LI>
</UL>

		  

		  
		

		  
			  
<UL>
    <LI>
    <B style="color: #224b8d">[11] Step-Back Profiling: Distilling User History for Personalized Scientific Writing</B><BR>
     <B>Xiangru Tang</B><span style="color: gray;">, Xingyao Zhang, Yanjun Shao, Jie Wu, Yilun Zhao, Arman Cohan, Ming Gong, Dongmei Zhang, Mark Gerstein.</span><BR>
	
	  <B>  IJCAI 2024 Workshop on AI4Research (Best Paper Award) </B>  <BR>
      [<a href="https://arxiv.org/abs/2406.14275">PDF</a>]
      [<a href="javascript:toggleBibtex('Profiling_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('Profiling_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/gersteinlab/step-back-profiling" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star gersteinlab/step-back-profiling on GitHub">Step-Back Profiling</a>	   
	
	<div id=Profiling_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Large language models (LLMs) excel at a variety of natural language processing tasks, yet they struggle to generate personalized content for individuals, particularly in real-world scenarios like scientific writing. Addressing this challenge, we introduce Step-Back Profiling to personalize LLMs by distilling user history into concise profiles, including essential traits and preferences of users. Regarding our experiments, we construct a Personalized Scientific Writing (PSW) dataset to study multiuser personalization. PSW requires the models to write scientific papers given specialized author groups with diverse academic backgrounds. As for the results, we demonstrate the effectiveness of capturing user characteristics via Step-Back Profiling for collaborative writing. Moreover, our approach outperforms the baselines by up to 3.6 points on the general personalization benchmark (LaMP), including 7 personalization LLM tasks. Our extensive ablation studies validate the contributions of different components in our method and provide insights into our task definition.
			    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=Profiling_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{tang2024step,
  title={Step-Back Profiling: Distilling User History for Personalized Scientific Writing},
  author={Xiangru Tang and Xingyao Zhang and Yanjun Shao and Jie Wu and Yilun Zhao and Arman Cohan and Ming Gong and Dongmei Zhang and Mark Gerstein},
  journal={arXiv preprint arXiv:2406.14275},
  year={2024}
} </pre>
       </div>
    </LI>
</UL>


	
		    


	  <UL>
    <LI>
    <B style="color: #224b8d">[10] A Survey of Generative AI for De Novo Drug Design: New Frontiers in Molecule and Protein Generation</B><BR>
	 <B>Xiangru Tang</B><span style="color: gray;">*, Howard Dai*, Elizabeth Knight*, Fang Wu, Yunyang Li, Tianxiao Li, Mark Gerstein.</span> <BR>
      <B>Briefings in Bioinformatics, 2024</B> (IF 13.99, JCR "Q1")<BR>
 <font color="#A6192E">"An introductory overview with a clear breakdown of datasets, benchmarks, & models."</font> <a href="https://x.com/XiangruTang/status/1757942770319360151" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:20px;"></i></a><br>

      [<a href="https://academic.oup.com/bib/article/25/4/bbae338/7713723">PDF</a>]
      [<a href="javascript:toggleBibtex('GAISurvey_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('GAISurvey_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/gersteinlab/GenAI4Drug" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star gersteinlab/GenAI4Drug on GitHub">GenAI4Drug</a>	   

	<div id=GAISurvey_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Artificial intelligence (AI)-driven methods can vastly improve the historically costly drug design process, with various generative models already in widespread use. Generative models for de novo drug design, in particular, focus on the creation of novel biological compounds entirely from scratch, representing a promising future direction. Rapid development in the field, combined with the inherent complexity of the drug design process, creates a difficult landscape for new researchers to enter. In this survey, we organize de novo drug design into two overarching themes: small molecule and protein generation. Within each theme, we identify a variety of subtasks and applications, highlighting important datasets, benchmarks, and model architectures and comparing the performance of top models. We take a broad approach to AI-driven drug design, allowing for both micro-level comparisons of various methods within each subtask and macro-level observations across different fields. We discuss parallel challenges and approaches between the two applications and highlight future directions for AI-driven de novo drug design as a whole. An organized repository of all covered sources is available at https://github.com/gersteinlab/GenAI4Drug.
			    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=GAISurvey_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{tang2024survey,
  title={A survey of generative ai for de novo drug design: new frontiers in molecule and protein generation},
  author={Tang, Xiangru and Dai, Howard and Knight, Elizabeth and Wu, Fang and Li, Yunyang and Li, Tianxiao and Gerstein, Mark},
  journal={Briefings in Bioinformatics},
  volume={25},
  number={4},
  year={2024},
  publisher={Oxford Academic}
}  </pre>
       </div>
    </LI>
</UL>

		  	  <UL>
    <LI>
    <B style="color: #224b8d">[9] MolLM: A Unified Language Model for Integrating Biomedical Text with 2D and 3D Molecular Representations</B><BR>
      <B>Xiangru Tang</B><span style="color: gray;">, Andrew Tran, Jeffrey Tan, Mark Gerstein.</span><BR>
      <B> ISMB 2024, Proceedings in Bioinformatics </B> (IF 6.93, JCR "Q1") <BR>
      [<a href="https://academic.oup.com/bioinformatics/article/40/Supplement_1/i357/7700902">PDF</a>]
      [<a href="javascript:toggleBibtex('MolLM_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('MolLM_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/gersteinlab/MolLM" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star gersteinlab/MolLM on GitHub">MolLM</a>	   
	
	<div id=MolLM_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
The present paradigm of deep learning models for molecular representation relies mostly on 1D or 2D formats, neglecting significant 3D structural information that offers valuable physical insight. This narrow focus inhibits the model's versatility and adaptability across a wide range of modalities. Conversely, the smaller amount of research that focuses on explicit 3D representation tends to overlook textual data within the biomedical domain. We present a unified pre-trained language model that concurrently captures biomedical text, 2D, and 3D molecular information.  Our model (the three-modality molecular language model, MolLM) consists of a text Transformer encoder and a molecular Transformer encoder, which encodes both 2D and 3D molecular structures. Our approach employs contrastive learning as a supervisory signal for cross-modal information learning, and we assemble a multimodality dataset using cheminformatics-based molecular modifications and a wealth of chemical text. MolLM demonstrates robust molecular representation capabilities in numerous downstream tasks, including cross-modality molecule and text matching, property prediction, captioning, and text-prompted editing. Through ablating the 3D functionality of our model, we demonstrate that the inclusion of text, 2D, and 3D representations significantly improves performance on the downstream tasks. Our code, data, and pre-trained model weights are all available at https://github.com/gersteinlab/MolLM.
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=MolLM_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{10.1093/bioinformatics/btae260,
    author = {Tang, Xiangru and Tran, Andrew and Tan, Jeffrey and Gerstein, Mark B},
    title = "{MolLM: a unified language model for integrating biomedical text with 2D and 3D molecular representations}",
    journal = {Bioinformatics},
    volume = {40},
    number = {Supplement_1},
    pages = {i357-i368},
    year = {2024},
    month = {06},
    abstract = "{The current paradigm of deep learning models for the joint representation of molecules and text primarily relies on 1D or 2D molecular formats, neglecting significant 3D structural information that offers valuable physical insight. This narrow focus inhibits the models’ versatility and adaptability across a wide range of modalities. Conversely, the limited research focusing on explicit 3D representation tends to overlook textual data within the biomedical domain.We present a unified pre-trained language model, MolLM, that concurrently captures 2D and 3D molecular information alongside biomedical text. MolLM consists of a text Transformer encoder and a molecular Transformer encoder, designed to encode both 2D and 3D molecular structures. To support MolLM’s self-supervised pre-training, we constructed 160K molecule-text pairings. Employing contrastive learning as a supervisory signal for learning, MolLM demonstrates robust molecular representation capabilities across four downstream tasks, including cross-modal molecule and text matching, property prediction, captioning, and text-prompted molecular editing. Through ablation, we demonstrate that the inclusion of explicit 3D representations improves performance in these downstream tasks.Our code, data, pre-trained model weights, and examples of using our model are all available at https://github.com/gersteinlab/MolLM. In particular, we provide Jupyter Notebooks offering step-by-step guidance on how to use MolLM to extract embeddings for both molecules and text.}",
    issn = {1367-4811},
    doi = {10.1093/bioinformatics/btae260},
    url = {https://doi.org/10.1093/bioinformatics/btae260},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/40/Supplement\_1/i357/58355106/btae260.pdf},
} </pre>
       </div>
    </LI>
</UL>




		  
		  
<UL>
    <LI>
    <B style="color: #224b8d">[8] BioCoder: A Benchmark for Bioinformatics Code Generation with Large Language Models


</B><BR>

<B>Xiangru Tang</B><span style="color: gray;">, Bill Qian, Rick Gao, Jiakang Chen, Xinyun Chen, Mark Gerstein.</span>

<BR>
      <B> ISMB 2024, Proceedings in Bioinformatics </B> (IF 6.93, JCR "Q1") <BR>
	    <font color="#A6192E">"BioCoder input covers repository-level potential package dependencies, class declarations, & global variables."</font> <a href="https://x.com/_akhaliq/status/1697449549369942360" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:20px;"></i></a><br>

      [<a href="https://academic.oup.com/bioinformatics/article/40/Supplement_1/i266/7700865">PDF</a>]
      [<a href="javascript:toggleBibtex('BioCoder_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('BioCoder_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/gersteinlab/biocoder" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star gersteinlab/biocoder on GitHub">BioCoder</a>	   
	<div id=BioCoder_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+, and ChatGPT. Our detailed analysis of these models emphasizes the importance of domain knowledge, pragmatic code generation, and contextual understanding. Our dataset, benchmark, Docker images, and scripts required for testing are all available at https://github.com/gersteinlab/biocoder.
		    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=BioCoder_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{10.1093/bioinformatics/btae230,
    author = {Tang, Xiangru and Qian, Bill and Gao, Rick and Chen, Jiakang and Chen, Xinyun and Gerstein, Mark B},
    title = "{BioCoder: a benchmark for bioinformatics code generation with large language models}",
    journal = {Bioinformatics},
    volume = {40},
    number = {Supplement_1},
    pages = {i266-i276},
    year = {2024},
    month = {06},
    abstract = "{Pretrained large language models (LLMs) have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks and to be appropriately specialized to particular domains. Here, we target bioinformatics due to the amount of domain knowledge, algorithms, and data operations this discipline requires. We present BioCoder, a benchmark developed to evaluate LLMs in generating bioinformatics-specific code. BioCoder spans much of the field, covering cross-file dependencies, class declarations, and global variables. It incorporates 1026 Python functions and 1243 Java methods extracted from GitHub, along with 253 examples from the Rosalind Project, all pertaining to bioinformatics. Using topic modeling, we show that the overall coverage of the included code is representative of the full spectrum of bioinformatics calculations. BioCoder incorporates a fuzz-testing framework for evaluation. We have applied it to evaluate various models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+, GPT-3.5, and GPT-4. Furthermore, we fine-tuned one model (StarCoder), demonstrating that our training dataset can enhance the performance on our testing benchmark (by \\&gt;15\\% in terms of Pass@K under certain prompt configurations and always \\&gt;3\\%). The results highlight two key aspects of successful models: (i) Successful models accommodate a long prompt (\\&gt;2600 tokens) with full context, including functional dependencies. (ii) They contain domain-specific knowledge of bioinformatics, beyond just general coding capability. This is evident from the performance gain of GPT-3.5/4 compared to the smaller models on our benchmark (50\\% versus up to 25\\%).All datasets, benchmark, Docker images, and scripts required for testing are available at: https://github.com/gersteinlab/biocoder and https://biocoder-benchmark.github.io/.}",
    issn = {1367-4811},
    doi = {10.1093/bioinformatics/btae230},
    url = {https://doi.org/10.1093/bioinformatics/btae230},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/40/Supplement\_1/i266/58354818/btae230.pdf},
} </pre>
       </div>
    </LI>
</UL>	

		  


<UL>
    <LI>
    <B style="color: #224b8d">[7] MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning
</B><BR>
<B>Xiangru Tang*</B><span style="color: gray;">, Anni Zou*, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, Arman Cohan, Mark Gerstein.</span><BR>
      <B>ACL 2024 Findings</B><BR>
<font color="#A6192E">"The first multi-agent framework within the medical context!"</font> <a href="https://x.com/XiangruTang/status/1793784334056374589" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:20px;"></i></a><br>

      [<a href="https://aclanthology.org/2024.findings-acl.33/">PDF</a>]
      [<a href="javascript:toggleBibtex('medagents_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('medagents_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/gersteinlab/MedAgents" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star gersteinlab/MedAgents on GitHub">MedAgents</a>	   
	
	<div id=medagents_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Large Language Models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and the reasoning over specialized knowledge. To address these obstinate issues, we propose a novel Multi-disciplinary Collaboration (MC) framework for the medical domain that leverages role-playing LLM-based agents who participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free and interpretable framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work particularly focuses on the zero-shot scenario, our results on nine data sets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MC framework excels at mining and harnessing the medical expertise in LLMs, as well as extending its reasoning abilities. Based on these outcomes, we further conduct a human evaluation to pinpoint and categorize common errors within our method, as well as ablation studies aimed at understanding the impact of various factors on overall performance.
			    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=medagents_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{tang2023medagents,
  title={MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning},
  author={Tang, Xiangru and Zou, Anni and Zhang, Zhuosheng and Zhao, Yilun and Zhang, Xingyao and Cohan, Arman and Gerstein, Mark},
  journal={arXiv preprint arXiv:2311.10537},
  year={2023}
}</pre>
       </div>
    </LI>
</UL>
	  

		  


<UL>
    <LI>
    <B style="color: #224b8d">[6] Struc-Bench: Are Large Language Models Good at Generating Complex Structured Tabular Data?


</B><BR>
<B>Xiangru Tang</B><span style="color: gray;">, Yiming Zong, Jason Phang, Yilun Zhao, Wangchunshu Zhou, Arman Cohan, Mark Gerstein.</span>

<BR>
      <B>NAACL 2024</B> (Oral)<BR>
      [<a href="https://arxiv.org/abs/2309.08963">PDF</a>]
      [<a href="javascript:toggleBibtex('Struc-Bench_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('Struc-Bench_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/gersteinlab/Struc-Bench" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star gersteinlab/Struc-Bench on GitHub">Struc-Bench</a>	   
	<div id=Struc-Bench_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Despite the remarkable capabilities of Large Language Models (LLMs) like GPT-4, producing complex, structured tabular data remains challenging. Our study assesses LLMs' proficiency in structuring tables and introduces a novel fine-tuning method, cognizant of data structures, to bolster their performance. We unveil Struc-Bench, a comprehensive benchmark featuring prominent LLMs (GPT-NeoX-20B, GPT-3.5, GPT-4, and Vicuna), which spans text tables, HTML, and LaTeX formats. Our proposed FormatCoT aids in crafting format-specific instructions from the intended outputs to populate this benchmark. Addressing the gap in task-centered evaluation, we propose two innovative metrics, P-Score (Prompting Score) and H-Score (Heuristical Score), to more accurately gauge LLM performance. Our experiments show that applying our structure-aware fine-tuning to LLaMA-7B leads to substantial performance gains, outshining its LLM counterparts across most measures. In-depth error analysis and creating an ability map across six dimensions, coverage, formatting, reasoning, comprehension, pragmatics, and hallucination, highlight areas for future enhancements and suggest forthcoming research trajectories. Our code and models can be found at https://github.com/gersteinlab/Struc-Bench.


		    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=Struc-Bench_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{tang-etal-2024-struc,
    title = "Struc-Bench: Are Large Language Models Good at Generating Complex Structured Tabular Data?",
    author = "Tang, Xiangru  and
      Zong, Yiming  and
      Phang, Jason  and
      Zhao, Yilun  and
      Zhou, Wangchunshu  and
      Cohan, Arman  and
      Gerstein, Mark",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-short.2",
    pages = "12--34",
    abstract = "Despite the remarkable capabilities of Large Language Models (LLMs) like GPT-4, producing complex, structured tabular data remains challenging. Our study assesses LLMs{'} proficiency in structuring tables and introduces a novel fine-tuning method, cognizant of data structures, to bolster their performance. We unveil Struc-Bench, a comprehensive benchmark featuring prominent LLMs (GPT-NeoX-20B, GPT-3.5, GPT-4, and Vicuna), which spans text tables, HTML, and LaTeX formats. Our proposed FormatCoT aids in crafting format-specific instructions from the intended outputs to populate this benchmark. Addressing the gap in task-centered evaluation, we propose two innovative metrics, P-Score (Prompting Score) and H-Score (Heuristical Score), to more accurately gauge LLM performance. Our experiments show that applying our structure-aware fine-tuning to LLaMA-7B leads to substantial performance gains, outshining its LLM counterparts across most measures. In-depth error analysis and creating an ability map across six dimensions, coverage, formatting, reasoning, comprehension, pragmatics, and hallucination, highlight areas for future enhancements and suggest forthcoming research trajectories. Our code and models can be found at https://github.com/gersteinlab/Struc-Bench.",
}</pre>
       </div>
    </LI>
</UL>


		  


<UL>
    <LI>
    <B style="color: #224b8d">[5] Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models


</B><BR>

<span style="color: gray;">Anni Zou, Zhuosheng Zhang, Hai Zhao,</span> <B>Xiangru Tang</B><span style="color: gray;">.</span>

<BR>
    <B>  IEEE Transactions on Audio, Speech and Language Processing</B> (In Review)<BR>
<font color="#A6192E">"Bridge the gap between performance and generalization when using the CoT prompting!"</font> <a href="https://x.com/omarsar0/status/1712835499256090972" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:20px;"></i></a><br>

      [<a href="https://arxiv.org/abs/2310.06692">PDF</a>]
      [<a href="javascript:toggleBibtex('Meta-CoT_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('Meta-CoT_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/Anni-Zou/Meta-CoT" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star Anni-Zou/Meta-CoT on GitHub">Meta-CoT</a>	   
	<div id=Meta-CoT_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Large language models (LLMs) have unveiled remarkable reasoning capabilities by exploiting chain-of-thought (CoT) prompting, which generates intermediate reasoning chains to serve as the rationale for deriving the answer. However, current CoT methods either simply employ general prompts such as Let's think step by step, or heavily rely on handcrafted task-specific demonstrations to attain preferable performances, thereby engendering an inescapable gap between performance and generalization. To bridge this gap, we propose Meta-CoT, a generalizable CoT prompting method in mixed-task scenarios where the type of input questions is unknown. Meta-CoT firstly categorizes the scenario based on the input question and subsequently constructs diverse demonstrations from the corresponding data pool in an automatic pattern. Meta-CoT simultaneously enjoys remarkable performances on ten public benchmark reasoning tasks and superior generalization capabilities. Notably, Meta-CoT achieves the state-of-the-art result on SVAMP (93.7%) without any additional program-aided methods. Our further experiments on five out-of-distribution datasets verify the stability and generality of Meta-CoT.
		    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=Meta-CoT_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{zou2023metacot,
  title={Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models},
  author={Anni Zou and Zhuosheng Zhang and Hai Zhao and Xiangru Tang},
  journal={arXiv preprint arXiv:2310.06692},
  year={2023}
} </pre>
       </div>
    </LI>
</UL>






	


<UL>
    <LI>
    <B style="color: #224b8d">

[4] Aligning Factual Consistency for Clinical Studies Summarization through Reinforcement Learning

</B><BR>

<B>Xiangru Tang</B><span style="color: gray;">, Arman Cohan, Mark Gerstein.</span>



<BR>
     <B>ACL 2023 Clinical Natural Language Processing</B> <BR>
      [<a href="https://aclanthology.org/2023.clinicalnlp-1.7/">PDF</a>]
      [<a href="javascript:toggleBibtex('Aligning_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('Aligning_bib')" target=_self>Bib</a>]
	<div id=Aligning_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
In the rapidly evolving landscape of medical research, accurate and concise summarization of clinical studies is crucial to support evidence-based practice. This paper presents a novel approach to clinical studies summarization, leveraging reinforcement learning to enhance factual consistency and align with human annotator preferences. Our work focuses on two tasks: Conclusion Generation and Review Generation. We train a CONFIT summarization model that outperforms GPT-3 and previous state-of-the-art models on the same datasets and collects expert and crowd-worker annotations to evaluate the quality and factual consistency of the generated summaries. These annotations enable us to measure the correlation of various automatic metrics, including modern factual evaluation metrics like QAFactEval, with human-assessed factual consistency. By employing top-correlated metrics as objectives for a reinforcement learning model, we demonstrate improved factuality in generated summaries that are preferred by human annotators.
			    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=Aligning_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{tang-etal-2023-aligning,
    title = "Aligning Factual Consistency for Clinical Studies Summarization through Reinforcement Learning",
    author = "Tang, Xiangru  and
      Cohan, Arman  and
      Gerstein, Mark",
    editor = "Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna",
    booktitle = "Proceedings of the 5th Clinical Natural Language Processing Workshop",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.clinicalnlp-1.7",
    doi = "10.18653/v1/2023.clinicalnlp-1.7",
    pages = "48--58",
    abstract = "In the rapidly evolving landscape of medical research, accurate and concise summarization of clinical studies is crucial to support evidence-based practice. This paper presents a novel approach to clinical studies summarization, leveraging reinforcement learning to enhance factual consistency and align with human annotator preferences. Our work focuses on two tasks: Conclusion Generation and Review Generation. We train a CONFIT summarization model that outperforms GPT-3 and previous state-of-the-art models on the same datasets and collects expert and crowd-worker annotations to evaluate the quality and factual consistency of the generated summaries. These annotations enable us to measure the correlation of various automatic metrics, including modern factual evaluation metrics like QAFactEval, with human-assessed factual consistency. By employing top-correlated metrics as objectives for a reinforcement learning model, we demonstrate improved factuality in generated summaries that are preferred by human annotators.",
}   </pre>
       </div>
    </LI>
</UL>





<UL>
    <LI>
    <B style="color: #224b8d">

[3] GersteinLab at MEDIQA-Chat 2023: Clinical Note Summarization from Doctor-Patient Conversations through Fine-tuning and In-context Learning
</B><BR>

<B>Xiangru Tang</B><span style="color: gray;">, Andrew Tran, Jeffrey Tan, Mark Gerstein.</span>



<BR>
     <B>ACL 2023 Clinical Natural Language Processing</B> <BR>
      [<a href="https://aclanthology.org/2023.clinicalnlp-1.58/">PDF</a>]
      [<a href="javascript:toggleBibtex('MEDIQA_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('MEDIQA_bib')" target=_self>Bib</a>]
	    <div style="padding-top:5px"> <a class="github-button" href="https://github.com/gersteinlab/MEDIQA-Chat-2023" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star gersteinlab/MEDIQA-Chat-2023 on GitHub">MEDIQA</a>	   
	<div id=confit_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
This paper presents our contribution to the MEDIQA-2023 Dialogue2Note shared task, encompassing both subtask A and subtask B. We approach the task as a dialogue summarization problem and implement two distinct pipelines: (a) a fine-tuning of a pre-trained dialogue summarization model and GPT-3, and (b) few-shot in-context learning (ICL) using a large language model, GPT-4. Both methods achieve excellent results in terms of ROUGE-1 F1, BERTScore F1 (deberta-xlarge-mnli), and BLEURT, with scores of 0.4011, 0.7058, and 0.5421, respectively. Additionally, we predict the associated section headers using RoBERTa and SciBERT based classification models. Our team ranked fourth among all teams, while each team is allowed to submit three runs as part of their submission. We also utilize expert annotations to demonstrate that the notes generated through the ICL GPT-4 are better than all other baselines.
			    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=MEDIQA_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{tang-etal-2023-gersteinlab,
    title = "{G}erstein{L}ab at {MEDIQA}-Chat 2023: Clinical Note Summarization from Doctor-Patient Conversations through Fine-tuning and In-context Learning",
    author = "Tang, Xiangru  and
      Tran, Andrew  and
      Tan, Jeffrey  and
      Gerstein, Mark",
    editor = "Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna",
    booktitle = "Proceedings of the 5th Clinical Natural Language Processing Workshop",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.clinicalnlp-1.58",
    doi = "10.18653/v1/2023.clinicalnlp-1.58",
    pages = "546--554",
    abstract = "This paper presents our contribution to the MEDIQA-2023 Dialogue2Note shared task, encompassing both subtask A and subtask B. We approach the task as a dialogue summarization problem and implement two distinct pipelines: (a) a fine-tuning of a pre-trained dialogue summarization model and GPT-3, and (b) few-shot in-context learning (ICL) using a large language model, GPT-4. Both methods achieve excellent results in terms of ROUGE-1 F1, BERTScore F1 (deberta-xlarge-mnli), and BLEURT, with scores of 0.4011, 0.7058, and 0.5421, respectively. Additionally, we predict the associated section headers using RoBERTa and SciBERT based classification models. Our team ranked fourth among all teams, while each team is allowed to submit three runs as part of their submission. We also utilize expert annotations to demonstrate that the notes generated through the ICL GPT-4 are better than all other baselines. The code for our submission is available.",
}  </pre>
       </div>
    </LI>
</UL>






<UL>
    <LI>
     <B style="color: #224b8d">[2] CONFIT: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning



</B><BR>
<B>Xiangru Tang</B><span style="color: gray;">, Arjun Nair, Borui Wang, Bingyao Wang, Jai Desai, Aaron Wade, Haoran Li, Asli Celikyilmaz, Yashar Mehdad, Dragomir Radev.</span>

<BR>
      <B>NAACL 2022</B> (Oral)<BR>
      [<a href="https://aclanthology.org/2022.naacl-main.415/">PDF</a>]
      [<a href="javascript:toggleBibtex('confit_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('confit_bib')" target=_self>Bib</a>]
	<div id=confit_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Factual inconsistencies in generated summaries severely limit the practical applications of abstractive dialogue summarization. Although significant progress has been achieved by using pre-trained neural language models, substantial amounts of hallucinated content are found during the human evaluation. In this work, we first devised a typology of factual errors to better understand the types of hallucinations generated by current models and conducted human evaluation on popular dialog summarization dataset. We further propose a training strategy that improves the factual consistency and overall quality of summaries via a novel contrastive fine-tuning, called CONFIT. To tackle top factual errors from our annotation, we introduce additional contrastive loss with carefully designed hard negative samples and self-supervised dialogue-specific loss to capture the key information between speakers. We show that our model significantly reduces all kinds of factual errors on both SAMSum dialogue summarization and AMI meeting summarization. On both datasets, we achieve significant improvements over state-of-the-art baselines using both automatic metrics, ROUGE and BARTScore, and human evaluation.


		    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=confit_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{tang-etal-2022-confit,
    title = "{CONFIT}: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning",
    author = "Tang, Xiangru  and
      Nair, Arjun  and
      Wang, Borui  and
      Wang, Bingyao  and
      Desai, Jai  and
      Wade, Aaron  and
      Li, Haoran  and
      Celikyilmaz, Asli  and
      Mehdad, Yashar  and
      Radev, Dragomir",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.415",
    doi = "10.18653/v1/2022.naacl-main.415",
    pages = "5657--5668",
}</pre>
       </div>
    </LI>
</UL>

		  

<UL>
    <LI>
     <B style="color: #224b8d">[1] Investigating Crowdsourcing Protocols for Evaluating the Factual Consistency of Summaries



</B><BR>
<B>Xiangru Tang</B><span style="color: gray;">, Alexander Fabbri, Haoran Li, Ziming Mao, Griffin Adams, Borui Wang, Asli Celikyilmaz, Yashar Mehdad, Dragomir Radev.</span>

<BR>
      <B>NAACL 2022</B><BR>
      [<a href="https://aclanthology.org/2022.naacl-main.417/">PDF</a>]
      [<a href="javascript:toggleBibtex('Investigating_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('Investigating_bib')" target=_self>Bib</a>]
	<div id=Investigating_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Current pre-trained models applied for summarization are prone to factual inconsistencies that misrepresent the source text. Evaluating the factual consistency of summaries is thus necessary to develop better models. However, the human evaluation setup for evaluating factual consistency has not been standardized. To determine the factors that affect the reliability of the human evaluation, we crowdsource evaluations for factual consistency across state-of-the-art models on two news summarization datasets using the rating-based Likert Scale and ranking-based Best-Worst Scaling. Our analysis reveals that the ranking-based Best-Worst Scaling offers a more reliable measure of summary quality across datasets and that the reliability of Likert ratings highly depends on the target dataset and the evaluation design. To improve crowdsourcing reliability, we extend the scale of the Likert rating and present a scoring algorithm for Best-Worst Scaling that we call value learning. Our crowdsourcing guidelines will be publicly available to facilitate future work on factual consistency in summarization.


		    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=Investigating_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{tang-etal-2022-investigating,
    title = "Investigating Crowdsourcing Protocols for Evaluating the Factual Consistency of Summaries",
    author = "Tang, Xiangru  and
      Fabbri, Alexander  and
      Li, Haoran  and
      Mao, Ziming  and
      Adams, Griffin  and
      Wang, Borui  and
      Celikyilmaz, Asli  and
      Mehdad, Yashar  and
      Radev, Dragomir",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.417",
    doi = "10.18653/v1/2022.naacl-main.417",
    pages = "5680--5692",
}</pre>
       </div>
    </LI>
</UL>



















		  




		  

 <br>

           </div>
      </div>
   </article>
  </div>	  	  






	       <div class="post">
  <article>
    <div class="cv">
          <h2 id="papers"><font size="5" color="#0f4d92"><b>Other Publications</b></font></h2>
          <div>





 <UL>
    <LI>
    <B style="color: #224b8d">[9] MultiAgentBench : Evaluating the Collaboration and Competition of LLM agents</B><BR>
	   <span style="color: gray;">Kunlun Zhu, Hongyi Du, Zhaochen Hong, Xiaocheng Yang, Shuyi Guo, Zhe Wang, Zhenhailong Wang, Cheng Qian, </span><B>Xiangru Tang</B><span style="color: gray;">, Heng Ji, Jiaxuan You</span>
<BR>
      <B> ACL 2025 </B> <BR>
      [<a href="https://arxiv.org/abs/2503.01935">PDF</a>]
      [<a href="javascript:toggleBibtex('MultiAgentBench_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('MultiAgentBench_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/MultiagentBench/MARBLE" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star MultiagentBench/MARBLE on GitHub">MultiAgentBench</a>	   

	<div id=MultiAgentBench_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, gpt-4o-mini reaches the average highest task score, graph structure performs the best among coordination protocols in the research scenario, and cognitive planning improves milestone achievement rates by 3%. 
			    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=MultiAgentBench_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{zhu2025multiagentbench,
  title={MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents},
  author={Zhu, Kunlun and Du, Hongyi and Hong, Zhaochen and Yang, Xiaocheng and Guo, Shuyi and Wang, Zhe and Wang, Zhenhailong and Qian, Cheng and Tang, Xiangru and Ji, Heng and others},
  journal={arXiv preprint arXiv:2503.01935},
  year={2025}
} </pre>
       </div>
    </LI>
</UL>







		  
 <UL>
    <LI>
    <B style="color: #224b8d">[8] OpenHands: An Open Platform for AI Software Developers as Generalist Agents</B><BR>
	   <span style="color: gray;">Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, </span><B>Xiangru Tang</B><span style="color: gray;">, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, Graham Neubig</span>
<BR>
      <B> ICLR 2025 </B> <BR>
	    <font color="#A6192E">"AI agents function as software developers, capable of command execution, web browsing & API interaction."</font> <a href="https://x.com/omarsar0/status/1816872317286281688" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:20px;"></i></a><br>

      [<a href="https://openreview.net/forum?id=OJd3ayDDoF">PDF</a>]
      [<a href="javascript:toggleBibtex('OpenHands_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('OpenHands_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/All-Hands-AI/OpenHands" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star All-Hands-AI/OpenHands on GitHub">OpenHands</a>	   

	<div id=OpenHands_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Software is one of the most powerful tools that we humans have at our disposal; it allows a skilled programmer to interact with the world in complex and profound ways. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and effect change in their surrounding environments. In this paper, we introduce OpenHands, a platform for the development of powerful and flexible AI agents that interact with the world in similar ways to a human developer: by writing code, interacting with a command line, and browsing the web. We describe how the platform allows for the implementation of new agents, utilization of various LLMs, safe interaction with sandboxed environments for code execution, and incorporation of evaluation benchmarks. Based on our currently incorporated benchmarks, we perform an evaluation of agents over 13 challenging tasks, including software engineering (e.g., SWE-Bench) and web browsing (e.g., WebArena), amongst others. Released under the permissive MIT license, OpenHands is a community project spanning academia and industry with more than 2K contributions from over 186 contributors in less than six months of development, and will improve going forward.
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=OpenHands_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{wang2025openhands,
      title={{OpenHands: An Open Platform for AI Software Developers as Generalist Agents}},
      author={Xingyao Wang and Boxuan Li and Yufan Song and Frank F. Xu and Xiangru Tang and Mingchen Zhuge and Jiayi Pan and Yueqi Song and Bowen Li and Jaskirat Singh and Hoang H. Tran and Fuqiang Li and Ren Ma and Mingzhang Zheng and Bill Qian and Yanjun Shao and Niklas Muennighoff and Yizhe Zhang and Binyuan Hui and Junyang Lin and Robert Brennan and Hao Peng and Heng Ji and Graham Neubig},
  booktitle={The Thirteenth International Conference on Learning Representations}
} </pre>
       </div>
    </LI>
</UL>


		  
  
		  <UL>
    <LI>
    <B style="color: #224b8d">[7] Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents</B><BR>
	  <span style="color: gray;">Zhuosheng Zhang, Yao Yao, Aston Zhang,</span> <B>Xiangru Tang</B><span style="color: gray;">, Xinbei Ma, Zhiwei He, Yiming Wang, Mark Gerstein, Rui Wang, Gongshen Liu, Hai Zhao.</span>
<BR>
            <B>ACM Computing Surveys, 2024</B> (IF 23.8)<BR>
	    	    <font color="#A6192E">"Generalization, efficiency, customization, scaling, and safety related to CoT and agents."</font> <a href="https://x.com/omarsar0/status/1726803725220487277" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:20px;"></i></a><br>

      [<a href="https://dl.acm.org/doi/10.1145/3719341">PDF</a>]
      [<a href="javascript:toggleBibtex('Igniting_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('Igniting_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/Zoeyyao27/CoT-Igniting-Agent" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star Zoeyyao27/CoT-Igniting-Agent on GitHub">CoT-Igniting-Agent</a>	   

	<div id=Igniting_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Large language models (LLMs) have dramatically enhanced the field of language intelligence, as demonstrably evidenced by their formidable empirical performance across a spectrum of complex reasoning tasks. Additionally, theoretical proofs have illuminated their emergent reasoning capabilities, providing a compelling showcase of their advanced cognitive abilities in linguistic contexts. Critical to their remarkable efficacy in handling complex reasoning tasks, LLMs leverage the intriguing chain-of-thought (CoT) reasoning techniques, obliging them to formulate intermediate steps en route to deriving an answer. The CoT reasoning approach has not only exhibited proficiency in amplifying reasoning performance but also in enhancing interpretability, controllability, and flexibility. In light of these merits, recent research endeavors have extended CoT reasoning methodologies to nurture the development of autonomous language agents, which adeptly adhere to language instructions and execute actions within varied environments. This survey paper orchestrates a thorough discourse, penetrating vital research dimensions, encompassing: (i) the foundational mechanics of CoT techniques, with a focus on elucidating the circumstances and justification behind its efficacy; (ii) the paradigm shift in CoT; and (iii) the burgeoning of language agents fortified by CoT approaches. Prospective research avenues envelop explorations into generalization, efficiency, customization, scaling, and safety. This paper caters to a wide audience, including beginners seeking comprehensive knowledge of CoT reasoning and language agents, as well as experienced researchers interested in foundational mechanics and engaging in cutting-edge discussions on these topics.                             </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=Igniting_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{zhang2025igniting,
  title={Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents},
  author={Zhang, Zhuosheng and Yao, Yao and Zhang, Aston and Tang, Xiangru and Ma, Xinbei and He, Zhiwei and Wang, Yiming and Gerstein, Mark and Wang, Rui and Liu, Gongshen and others},
  journal={ACM Computing Surveys},
  year={2025}
}  </pre>
       </div>
    </LI>
</UL>



		

		  
		  <UL>
    <LI>
    <B style="color: #224b8d">[6] Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity</B><BR>
	   <span style="color: gray;">Cunxiang Wang*, Xiaoze Liu*, Yuanhao Yue*,</span> <B>Xiangru Tang</B><span style="color: gray;">, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang, Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang, Yue Zhang.</span>
<BR>
            <B>ACM Computing Surveys, 2024</B> (IF 23.8)<BR>
      [<a href="https://arxiv.org/abs/2310.07521">PDF</a>]
      [<a href="javascript:toggleBibtex('Factuality_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('Factuality_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/wangcunxiang/LLM-Factuality-Survey" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star wangcunxiang/LLM-Factuality-Survey on GitHub">LLM-Factuality-Survey</a>	   

	<div id=Factuality_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
This survey addresses the crucial issue of factuality in Large Language Models (LLMs). As LLMs find applications across diverse domains, the reliability and accuracy of their outputs become vital. We define the Factuality Issue as the probability of LLMs to produce content inconsistent with established facts. We first delve into the implications of these inaccuracies, highlighting the potential consequences and challenges posed by factual errors in LLM outputs. Subsequently, we analyze the mechanisms through which LLMs store and process facts, seeking the primary causes of factual errors. Our discussion then transitions to methodologies for evaluating LLM factuality, emphasizing key metrics, benchmarks, and studies. We further explore strategies for enhancing LLM factuality, including approaches tailored for specific domains. We focus two primary LLM configurations standalone LLMs and Retrieval-Augmented LLMs that utilizes external data, we detail their unique challenges and potential enhancements. Our survey offers a structured guide for researchers aiming to fortify the factual reliability of LLMs.
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=Factuality_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{wang2025survey,
  title={Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity},
  author={Wang, Cunxiang and Liu, Xiaoze and Yue, Yuanhao and Tang, Xiangru and Zhang, Tianhang and Jiayang, Cheng and Yao, Yunzhi and Gao, Wenyang and Hu, Xuming and Qi, Zehan and others},
  journal={ACM Computing Surveys},
  year={2025}
}  </pre>
       </div>
    </LI>
</UL>


 <UL>
    <LI>
    <B style="color: #224b8d">[5] OctoPack: Instruction Tuning Code Large Language Models</B><BR>
	  <span style="color: gray;">Niklas Muennighoff, Qian Liu, Armel Randy Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh,</span> <B>Xiangru Tang</B><span style="color: gray;">, Leandro Von Werra, Shayne Longpre.</span>
<BR>
            <B>ICLR 2024</B><BR>
      [<a href="https://openreview.net/forum?id=mw1PWNSWZP">PDF</a>]
      [<a href="javascript:toggleBibtex('OctoPack_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('OctoPack_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/bigcode-project/octopack" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star bigcode-project/octopack on GitHub">octopack</a>	   

	<div id=OctoPack_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=OctoPack_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{muennighoffoctopack,
  title={OctoPack: Instruction Tuning Code Large Language Models},
  author={Muennighoff, Niklas and Liu, Qian and Zebaze, Armel Randy and Zheng, Qinkai and Hui, Binyuan and Zhuo, Terry Yue and Singh, Swayam and Tang, Xiangru and Von Werra, Leandro and Longpre, Shayne},
  booktitle={The Twelfth International Conference on Learning Representations}
}</pre>
       </div>
    </LI>
</UL>


		  <UL>
    <LI>
    <B style="color: #224b8d">[4] ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs</B><BR>
	  <span style="color: gray;">Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong,</span> <B>Xiangru Tang</B><span style="color: gray;">, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, dahai li, Zhiyuan Liu, Maosong Sun.</span>
<BR>
            <B>ICLR 2024</B><BR>
      [<a href="https://openreview.net/forum?id=dHng2O0Jjr">PDF</a>]
      [<a href="javascript:toggleBibtex('ToolLLM_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('ToolLLM_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/OpenBMB/ToolBench" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star OpenBMB/ToolBench on GitHub">ToolBench</a>	   

	<div id=OctoPack_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=OctoPack_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{qin2023toolllm,
  title={Toolllm: Facilitating large language models to master 16000+ real-world apis},
  author={Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and others},
  journal={arXiv preprint arXiv:2307.16789},
  year={2023}
} </pre>
       </div>
    </LI>
</UL>




		  


		  

 <UL>
    <LI>
    <B style="color: #224b8d">[3] PRESTO: Progressive Pretraining Enhances Synthetic Chemistry Outcomes</B><BR>
	  <span style="color: gray;">He Cao, Yanjun Shao, Zhiyuan Liu, Zijing Liu,</span> <B>Xiangru Tang</B><span style="color: gray;">, Yuan Yao, Yu Li.</span>
<BR>
            <B>EMNLP 2024 Findings</B><BR>
      [<a href="https://aclanthology.org/2024.acl-long.852">PDF</a>]
      [<a href="javascript:toggleBibtex('PRESTO_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('PRESTO_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/IDEA-XL/PRESTO" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star IDEA-XL/PRESTO on GitHub">PRESTO</a>	   

	<div id=PRESTO_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Multimodal Large Language Models (MLLMs) have seen growing adoption across various scientific disciplines. These advancements encourage the investigation of molecule-text modeling within synthetic chemistry, a field dedicated to designing and conducting chemical reactions to synthesize new compounds with desired properties and applications. Current approaches, however, often neglect the critical role of multi-molecule graph interaction in understanding chemical reactions, leading to suboptimal performance in synthetic chemistry tasks. This study introduces PRESTO (Progressive Pretraining Enhances Synthetic Chemistry Outcomes), a new framework that bridges the molecule-text modality gap by integrating a comprehensive benchmark of pretraining strategies and dataset configurations. It progressively improves multimodal LLMs through cross-modal alignment and multi-graph understanding. Our extensive experiments demonstrate that PRESTO offers competitive results in downstream synthetic chemistry tasks. The code can be found at https://github.com/IDEA-XL/PRESTO.
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=PRESTO_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{cao-etal-2024-presto,
    title = "{PRESTO}: Progressive Pretraining Enhances Synthetic Chemistry Outcomes",
    author = "Cao, He  and
      Shao, Yanjun  and
      Liu, Zhiyuan  and
      Liu, Zijing  and
      Tang, Xiangru  and
      Yao, Yuan  and
      Li, Yu",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.597/",
    doi = "10.18653/v1/2024.findings-emnlp.597",
    pages = "10197--10224",
    abstract = "Multimodal Large Language Models (MLLMs) have seen growing adoption across various scientific disciplines. These advancements encourage the investigation of molecule-text modeling within synthetic chemistry, a field dedicated to designing and conducting chemical reactions to synthesize new compounds with desired properties and applications. Current approaches, however, often neglect the critical role of multi-molecule graph interaction in understanding chemical reactions, leading to suboptimal performance in synthetic chemistry tasks. This study introduces PRESTO (Progressive Pretraining Enhances Synthetic Chemistry Outcomes), a new framework that bridges the molecule-text modality gap by integrating a comprehensive benchmark of pretraining strategies and dataset configurations. It progressively improves multimodal LLMs through cross-modal alignment and multi-graph understanding. Our extensive experiments demonstrate that PRESTO offers competitive results in downstream synthetic chemistry tasks. The code can be found at https://github.com/IDEA-XL/PRESTO."
} </pre>
       </div>
    </LI>
</UL>



 <UL>
    <LI>
    <B style="color: #224b8d">[2] Investigating Data Contamination in Modern Benchmarks for Large Language Models</B><BR>
	  <span style="color: gray;">Chunyuan Deng, Yilun Zhao,</span> <B>Xiangru Tang</B><span style="color: gray;">, Mark Gerstein, Arman Cohan.</span>
<BR>
            <B>NAACL 2024</B><BR>
      [<a href="https://aclanthology.org/2024.naacl-long.482">PDF</a>]
      [<a href="javascript:toggleBibtex('Contamination_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('Contamination_bib')" target=_self>Bib</a>]

	<div id=Contamination_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Recent observations have underscored a disparity between the inflated benchmark scores and the actual performance of LLMs, raising concerns about potential contamination of evaluation benchmarks. This issue is especially critical for closed-source models and certain open-source models where training data transparency is lacking. In this paper we study data contamination by proposing two methods tailored for both open-source and proprietary LLMs. We first introduce a retrieval-based system to explore potential overlaps between evaluation benchmarks and pretraining corpora. We further present a novel investigation protocol named Testset Slot Guessing (TS-Guessing), applicable to both open and proprietary models. This approach entails masking a wrong answer in a multiple-choice question and prompting the model to fill in the gap. Additionally, it involves obscuring an unlikely word in an evaluation example and asking the model to produce it. We find that certain commercial LLMs could surprisingly guess the missing option in various test sets. Specifically, in the MMLU benchmark, ChatGPT and GPT-4 demonstrated an exact match rate of 52% and 57%, respectively, in guessing the missing options in benchmark test data. We hope these results underscore the need for more robust evaluation methodologies and benchmarks in the field.
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=Contamination_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{deng-etal-2024-investigating,
    title = "Investigating Data Contamination in Modern Benchmarks for Large Language Models",
    author = "Deng, Chunyuan  and
      Zhao, Yilun  and
      Tang, Xiangru  and
      Gerstein, Mark  and
      Cohan, Arman",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.482/",
    doi = "10.18653/v1/2024.naacl-long.482",
    pages = "8706--8719",
    abstract = "Recent observations have underscored a disparity between the inflated benchmark scores and the actual performance of LLMs, raising concerns about potential contamination of evaluation benchmarks. This issue is especially critical for closed-source models and certain open-source models where training data transparency is lacking. In this paper we study data contamination by proposing two methods tailored for both open-source and proprietary LLMs. We first introduce a retrieval-based system to explore potential overlaps between evaluation benchmarks and pretraining corpora. We further present a novel investigation protocol named Testset Slot Guessing (TS-Guessing), applicable to both open and proprietary models. This approach entails masking a wrong answer in a multiple-choice question and prompting the model to fill in the gap. Additionally, it involves obscuring an unlikely word in an evaluation example and asking the model to produce it. We find that certain commercial LLMs could surprisingly guess the missing option in various test sets. Specifically, in the MMLU benchmark, ChatGPT and GPT-4 demonstrated an exact match rate of 52{\%} and 57{\%}, respectively, in guessing the missing options in benchmark test data. We hope these results underscore the need for more robust evaluation methodologies and benchmarks in the field."
} </pre>
       </div>
    </LI>
</UL>




 <UL>
    <LI>
    <B style="color: #224b8d">[1] FAVOR-GPT: a generative natural language interface to whole genome variant functional annotations</B><BR>
	  <span style="color: gray;">Thomas Cheng Li, Hufeng Zhou, Vineet Verma,</span> <B>Xiangru Tang</B><span style="color: gray;">, Yanjun Shao, Eric Van Buren, Zhiping Weng, Mark Gerstein, Benjamin Neale, Shamil R Sunyaev, Xihong Lin.</span>
<BR>
            <B>Bioinformatics Advances, 2024</B> (IF 2.32, JCR Q2)<BR>
      [<a href="https://academic.oup.com/bioinformaticsadvances/article/4/1/vbae143/7789482">PDF</a>]
      [<a href="javascript:toggleBibtex('FAVOR_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('FAVOR_bib')" target=_self>Bib</a>]

	<div id=FAVOR_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Functional Annotation of genomic Variants Online Resources (FAVOR) offers multi-faceted, whole genome variant functional annotations, which is essential for Whole Genome and Exome Sequencing (WGS/WES) analysis and the functional prioritization of disease-associated variants. A versatile chatbot designed to facilitate informative interpretation and interactive, user-centric summary of the whole genome variant functional annotation data in the FAVOR database is needed. We have developed FAVOR-GPT, a generative natural language interface powered by integrating large language models (LLMs) and FAVOR. It is developed based on the Retrieval Augmented Generation (RAG) approach, and complements the original FAVOR portal, enhancing usability for users, especially those without specialized expertise. FAVOR-GPT simplifies raw annotations by providing interpretable explanations and result summaries in response to the user’s prompt. It shows high accuracy when cross-referencing with the FAVOR database, underscoring the robustness of the retrieval framework.                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=FAVOR_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{10.1093/bioadv/vbae143,
    author = {Li, Thomas Cheng and Zhou, Hufeng and Verma, Vineet and Tang, Xiangru and Shao, Yanjun and Van Buren, Eric and Weng, Zhiping and Gerstein, Mark and Neale, Benjamin and Sunyaev, Shamil R and Lin, Xihong},
    title = {FAVOR-GPT: a generative natural language interface to whole genome variant functional annotations},
    journal = {Bioinformatics Advances},
    volume = {4},
    number = {1},
    pages = {vbae143},
    year = {2024},
    month = {09},
    abstract = {Functional Annotation of genomic Variants Online Resources (FAVOR) offers multi-faceted, whole genome variant functional annotations, which is essential for Whole Genome and Exome Sequencing (WGS/WES) analysis and the functional prioritization of disease-associated variants. A versatile chatbot designed to facilitate informative interpretation and interactive, user-centric summary of the whole genome variant functional annotation data in the FAVOR database is needed.We have developed FAVOR-GPT, a generative natural language interface powered by integrating large language models (LLMs) and FAVOR. It is developed based on the Retrieval Augmented Generation (RAG) approach, and complements the original FAVOR portal, enhancing usability for users, especially those without specialized expertise. FAVOR-GPT simplifies raw annotations by providing interpretable explanations and result summaries in response to the user’s prompt. It shows high accuracy when cross-referencing with the FAVOR database, underscoring the robustness of the retrieval framework.Researchers can access FAVOR-GPT at FAVOR’s main website (https://favor.genohub.org).},
    issn = {2635-0041},
    doi = {10.1093/bioadv/vbae143},
    url = {https://doi.org/10.1093/bioadv/vbae143},
    eprint = {https://academic.oup.com/bioinformaticsadvances/article-pdf/4/1/vbae143/59645690/vbae143.pdf},
} </pre>
       </div>
    </LI>
</UL>




 <br>

           </div>
      </div>
   </article>
  </div>














	      
	      
		      		  
  
<div class="post">
  <article>
    <div class="cv">
          <h2 id="talks"><font size="5" color="#0f4d92"><b>Recent Talks</b></font></h2>
          <div>


		  		  


		  
 <strong>07/2025 </strong> Talk at ISMB 2025 <a href="https://www.iscb.org/ismbeccb2025/programme-agenda/scientific-programme/3dsig" target="_blank" rel="noopener noreferrer">3DSIG Section</a>.
 <br>
		  
 <strong>11/2024 </strong> Talk at <a href="files/Robert Tang CoP Speaker Series.pdf" target="_blank" rel="noopener noreferrer">Takeda Pharmaceutical</a>.

 <br>
		  
  <strong>07/2024 </strong> Talk at <a href="files/NLP IG Flyer 7-24-2024.pdf" target="_blank" rel="noopener noreferrer">Yale Department of Biomedical Informatics & Data Science</a>.



 <br>

		  
 <strong>07/2024 </strong> Talk at ISMB 2024 <a href="https://www.iscb.org/ismb2024/programme-schedule/scientific-programme/textmining" target="_blank" rel="noopener noreferrer">Text Mining Section</a>.

 <br>

 <strong>07/2024 </strong> Talk at <a href="https://www.mllm-ai.com/" target="_blank" rel="noopener noreferrer">Multimodal Large Language Model</a>.

 <br>

		  
 <strong>02/2024 </strong> Talk at <a href="https://medicine.yale.edu/event/inaugural-ai-in-medicine-symposium-at-ysm/" target="_blank" rel="noopener noreferrer">AI in Medicine Symposium</a> at Yale School of Medicine.

 <br>
 <strong>01/2024 </strong> Talk at PSB 2024 Workshop on <a href="https://www.ncbi.nlm.nih.gov/research/bionlp/psb2024" target="_blank" rel="noopener noreferrer">LLMs for Biomedicine</a>.

 <br>
 <strong>07/2023 </strong> Talk at ISMB/ECCB 2023 <a href="https://www.iscb.org/ismbeccb2023-programme/tracks/text-mining" target="_blank" rel="noopener noreferrer">Text Mining Section</a>.


		   <br>
 <br>




		  
           </div>
      </div>
   </article>
  </div>	  	  




	  	  









	      
	      
  <div class="post">
  <article>
    <div class="cv">
          <h2 id="services"><font size="5" color="#0f4d92"><b>Workshop & Tutorial Organizers</b></font></h2>
          <div>
		  

 <strong>Workshop Organizer:</strong> ICML 2025 Workshop on <a href="https://mas-2025.github.io/MAS-2025/" target="_blank" rel="noopener noreferrer">Multi-Agent Systems</a>, </strong> ICCV 2025 Workshop on <a href="https://knowledgemr-workshop.github.io/" target="_blank" rel="noopener noreferrer">Knowledge-Intensive Multimodal Reasoning</a>, </strong> ICLR 2024 Workshop on <a href="https://llmagents.github.io/" target="_blank" rel="noopener noreferrer">LLM Agents</a>, SIGDIAL/INLG 2023 Workshop on <a href="https://ctrlnlg.github.io/" target="_blank" rel="noopener noreferrer">Taming LLMs</a>.
 <br>

 		  <strong>Tutorial Organizer:</strong> ISMB 2024 Tutorial on <a href="https://llm4biomed.github.io/" target="_blank" rel="noopener noreferrer">A Practical Introduction to LLMs in Biomedical Research</a>.
 <br>
 		  <strong>Session Chair:</strong> ACL 2024 BoF on AI for Science, NAACL 2024 BoF on <a href="https://llm4sci.github.io/" target="_blank" rel="noopener noreferrer">LLMs for Science</a>.

		   <br>
 <br>

           </div>
      </div>
   </article>
  </div>	  	  




	      
<div class="post">
  <article>
    <div class="cv">
          <h2 id="services"><font size="5" color="#0f4d92"><b>Services</b></font></h2>
          <div>
		  
 
  <strong>Area Chair:</strong> <a href="https://aclrollingreview.org/dates" target="_blank" rel="noopener noreferrer">ACL ARR</a> (ACL, EMNLP, NAACL, etc).
 <br>	  
  
 <strong>Conference Program Committee / Reviewer</strong>: NeurIPS, ICML, ACL, EMNLP, CIKM, NAACL, INLG, IEEE BigData, COLM.
<br>
 
 <strong>Journal Reviewer</strong>: npj Digital Medicine, TPAMI, Neurocomputing, Briefings in Bioinformatics, PLOS Computational Biology, BMC Bioinformatics, PLOS ONE, Health Data Science.
<br>
 <strong>Workshop Reviewer</strong>: KDD 2023 Workshop on Data Mining in Bioinformatics, ACL 2023 Workshop on Building Educational Apps, ACL 2023 Workshop on Clinical NLP, ICML 2023 Workshop on Neural Conv AI, ICML 2023 Workshop on Interpretable ML in Healthcare, NAACL-HLT 2021 Workshop on Language and Vision Research.

		   <br>
 <br>

           </div>
      </div>
   </article>
  </div>	  	  

	  

<div class="post">
  <article>
    <div class="cv">
          <h2 id="teaching"><font size="5" color="#0f4d92"><b>Teaching</b></font></h2>
          <div>

<strong>Teaching Fellow</strong> - CPSC 452/CPSC 552/AMTH 552/CB&B 663 Deep Learning Theory and Applications, <i>Yale University</i>, 2023 Spring.
 <br> 
<strong>Teaching Fellow</strong> - CPSC 437/CPSC 537 Introduction to Database Systems, <i>Yale University</i>, 2023 Fall.
 <br> 	  
<strong>Teaching Fellow</strong> - CPSC 452/CPSC 552/AMTH 552/CB&B 663 Deep Learning Theory and Applications, <i>Yale University</i>, 2024 Spring.
 <br> 
<strong>Teaching Fellow</strong> - CPSC 437/CPSC 537 Database Systems, <i>Yale University</i>, 2024 Fall.


		   <br>
		   <br>
          </div>
      </div>
  </article>
</div>


	 

<div class="post">
  <article>
    <div class="cv">
          <h2 id="misc"><font size="5" color="#0f4d92"><b>Misc.</b></font></h2>
          <div>

<strong>I took 12 courses (and 3 additional project credits) at Yale:</strong> CPSC 523 Principles of Operating Systems, 537 Intro to Database, 539 Software Engineering, 552 Deep Learning Theory, 553 Unsupervised Learning, 569 Randomized Algorithms, 577 NLP, 583 Deep Learning on Graph, 668 Blockchain Research, 677 Adv NLP, 680 Trustworthy Deep Learning, 752 Biomedical Data Sci.

		   <br>
		  Interestingly, this course load matches the requirement for a <a href="https://catalog.yale.edu/ycps/subjects-of-instruction/computer-science/" target="_blank" rel="noopener noreferrer">B.S. degree in Computer Science</a> (which requires 11 courses + 1 project credit) and exceeds what's needed for a B.A. (which requires only 9 courses + 1 project credit).
	   <br>
          </div>
      </div>
  </article>
</div>

	      
	      
	      
	      <!-- 进度条-->

<script type="text/javascript">
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>


	      
	      
	      
	      
<!-- Default Statcounter code for Homepage https://xiangrutang.github.io/ -->
<script type="text/javascript">
var sc_project=12795012; 
var sc_invisible=0; 
var sc_security="88264835"; 
var scJsHost = "https://";
document.write("<sc"+"ript type='text/javascript' src='" + scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img class="statcounter"
src="https://c.statcounter.com/12795012/0/88264835/0/" alt="Web Analytics"
referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
<!-- End of Statcounter Code -->

</body>
</html>

<!-- Localized -->
