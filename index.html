<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" >
  <meta name="description" content="Ph.D. student at Yale">
  <title>Xiangru (Robert) Tang - Yale</title>
  <link rel="shortcut icon" href="images/tree-12c-194370/tree-12c-152-194370.png" />
  <link href='assets/css_s/bootstrap.min.css' rel='stylesheet'>
  <link href='assets/css_s/main_style.css' rel='stylesheet'>
  <link href='https://fonts.googleapis.com/css?family=Nunito:200,300,400,500,600,700,800,900,100italic,100,300italic,400italic,500italic,900italic,700italic' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Lato:200,300,400,500,600,700,800,900,100italic,100,300italic,400italic,500italic,900italic,700italic' rel='stylesheet' type='text/css'>

<script src="http://cdn.static.runoob.com/libs/jquery/2.1.1/jquery.min.js"></script>
<script src="http://cdn.static.runoob.com/libs/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<link href="bootstrap.min.css" rel="stylesheet" media="screen" />
<link href="font-awesome-4.7.0/css/font-awesome.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
<LINK rel=stylesheet type=text/css  href="./jemdoc.css"><LINK rel="shortcut icon" href="rui.ico">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<script async defer src="buttons.js"></script>
<script src="jquery.scrollUp.js"></script>
<!-- Place this tag in your head or just before your close body tag. -->
<script async defer src="https://buttons.github.io/buttons.js"></script>
	
<SCRIPT type=text/javascript>
  
var a = document.querySelector(".navbar-toggle");
    $(".navbar-nav li a").on("click",function () {
        a.click();
    });

// Toggle Display of BibTeX
function toggleBibtex(articleid) {
  var bib = document.getElementById(articleid);
  // Toggle 
    if(bib.style.display == "none") {
      var x = document.getElementsByClassName("blockcontent");
      var i;
      for (i = 0; i < x.length; i++) {
          x[i].style.display = "none";
      }
      bib.style.display = "";
    }
    else {
      bib.style.display = "none";
    }

}

function showall(){  
    var items = document.getElementsByClassName('coauthor');
    if(document.getElementById("list_type").text == "Full list"){
        document.getElementById("list_type").text = "Selected";
        for (i = 0; i < items.length; ++i)
            {
                items[i].style.display = 'block';
            }
    }else{
        document.getElementById("list_type").text = "Full list";
        for (i = 0; i < items.length; ++i)
            {
                items[i].style.display = 'none';
            }
    }
    
}
</SCRIPT>

<script>
$(function () {
    $.scrollUp({
        animation: 'fade',
        scrollImg: {
            active: true,
            type: 'background',
            src: 'imgs/top.png'
        }
    });
});
$('#scrollUpTheme').attr('href', 'image.css?1.1');
$('.image-switch').addClass('active');

</script>
	
	

    <!-- Bootstrap & MDB
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />
 -->
    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>">
    
    <!-- <link rel="stylesheet" href="/assets/css/main.css"> -->
    <link rel="stylesheet" href="assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/">

    <!-- <script src="/assets/js/theme.js"></script> -->
    <script src="assets/js/theme.js"></script>
    <!-- <script src="/assets/js/dark_mode.js"></script> -->
    <script src="assets/js/dark_mode.js"></script>

	
	
	
	
	
	
</head>
<body>
  <div id='header' class ='bg'>
    <div id='header-inner'>
      <div id="author-avatar"></div>
      <div class='header-text'>
        <div class='header-text-name'>
            Robert Tang <span style="font-family:STFangsong; font-size:24pt"> </span>
        </div>
        <div class='header-text-email'>
          <p>Ph.D. Student</p>
          <p>Computer Science, Yale University, U.S.</p>
          <p>xiangru.[lastname]@yale.edu</p>
        </div>

        <div class='header-text-items'>
          <ul class="icons">
            <li><a href="https://scholar.google.com/citations?user=gGcRkpYAAAAJ" class="icon ai-google-scholar"><span class="label"></span></a></li>
	    <li><a href="https://cpsc.yale.edu/people/xiangru-tang" class="icon fa-home"><span class="label"></span></a></li>
            <li><a href="https://twitter.com/XiangruTang" class="icon fa-twitter"><span class="label"></span></a></li>
            <li><a href="https://github.com/tangxiangru" class="icon fa-github"><span class="label"></span></a></li>
           <!--  <li><a href="files/.pdf" class="icon fa-file-pdf-o"><span class="label"></span></a></li> -->
          </ul>
        </div>
      </div>
    </div>
  </div>

  <div class='container'>
    <div class='col-xs-1'>
    </div>
    <div class='col-xs-10'>
      <div class='row'>
	      

<p>I'm a second-year Ph.D. student in Computer Science at <a href="http://www.yale.edu/" target="_blank" rel="noopener noreferrer">Yale University</a>, where I am very fortunately advised by Prof. <a href="https://scholar.google.com/citations?user=YvjuUugAAAAJ" target="_blank" rel="noopener noreferrer">Mark Gerstein</a>. Previously, I got my master's from <a href="https://cpsc.yale.edu/" target="_blank" rel="noopener noreferrer">Yale CS</a> as well, advised by Prof. Dragomir Radev.
My research lies in the intersection of <b>large language models</b> and applications in <b>biology</b>.</p>
	      
<ul>
    <li>
 <strong>Large Language Models (LLMs):</strong> I study how LLMs can produce extremely complex and intelligent behaviors via multi-agent collaboration and the exploration of tool use strategies.

    </li>
    <li>
<strong>AI for Biology:</strong> I aim to design novel and useful biomolecules with LLMs, like In-Context Learning and Chain-of-Thought Prompting.

    </li>
    <li>
<strong>Code Generation:</strong> I aim to make programming more communicative, by creating models, methods, and datasets for producing code from language.
    </li>
</ul>

<!-- 
	      <p>My ultimate goal is to use AI to reduce some of the world’s worst inequities. Globally, the worst inequity is in the health and medical field. And my work is directed toward achieving this objective. </p>
<br>
 -->	
My research is supported by my advisor's Amazon Research Award.

 <br>
 <br>


	        
<div class="post">
  <article>
    <div class="cv">
          <h2 id="papers"><font size="5" color="#0f4d92"><b>Selected Publications</b></font></h2>
          <div>
	
Discover the  <a href="https://scholar.google.com/citations?hl=en&user=gGcRkpYAAAAJ&view_op=list_works&sortby=pubdate" target="_blank"><img src="./images/google_scholar.png" height="20" alt="google scholar"></a> | 
<a href="https://www.semanticscholar.org/author/Xiangru-Tang/47274259?sort=pub-date" target="_blank"><img src="./images/semantic_scholar_logo.png" height="20" alt="semantic scholar"></a> 


		  
 <br>
	   <br>

<UL>
    <LI>
    <B style="color: #224b8d">Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents</B><BR>
      Zhuosheng Zhang, Yao Yao, Aston Zhang, <B>Xiangru Tang</B>, Xinbei Ma, Zhiwei He, Yiming Wang, Mark Gerstein, Rui Wang, Gongshen Liu, Hai Zhao.<BR>
      arXiv, 2023<BR>
      [<a href="https://arxiv.org/abs/2311.11797">PDF</a>]
      [<a href="javascript:toggleBibtex('CoT-Igniting-Agent_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('CoT-Igniting-Agent_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/Zoeyyao27/CoT-Igniting-Agent" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star Zoeyyao27/CoT-Igniting-Agent on GitHub">CoT-Igniting-Agent</a>	   
	
	<div id=CoT-Igniting-Agent_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
                                Large language models (LLMs) have dramatically enhanced the field of language intelligence, as demonstrably evidenced by their formidable empirical performance across a spectrum of complex reasoning tasks. Additionally, theoretical proofs have illuminated their emergent reasoning capabilities, providing a compelling showcase of their advanced cognitive abilities in linguistic contexts. Critical to their remarkable efficacy in handling complex reasoning tasks, LLMs leverage the intriguing chain-of-thought (CoT) reasoning techniques, obliging them to formulate intermediate steps en route to deriving an answer. The CoT reasoning approach has not only exhibited proficiency in amplifying reasoning performance but also in enhancing interpretability, controllability, and flexibility. In light of these merits, recent research endeavors have extended CoT reasoning methodologies to nurture the development of autonomous language agents, which adeptly adhere to language instructions and execute actions within varied environments. This survey paper orchestrates a thorough discourse, penetrating vital research dimensions, encompassing: (i) the foundational mechanics of CoT techniques, with a focus on elucidating the circumstances and justification behind its efficacy; (ii) the paradigm shift in CoT; and (iii) the burgeoning of language agents fortified by CoT approaches. Prospective research avenues envelop explorations into generalization, efficiency, customization, scaling, and safety. We hope to offer readers a comprehensive understanding of prevalent research areas such as CoT reasoning and language agents and illuminate the interconnections weaving through these areas. This paper caters to a wide audience, including beginners seeking comprehensive knowledge of CoT reasoning and language agents, as well as experienced researchers interested in foundational mechanics and engaging in cutting-edge discussions on these topics. A repository for the related papers is available at https://github.com/Zoeyyao27/CoT-Igniting-Agent.
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=CoT-Igniting-Agent_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{zhang2023igniting,
  title={Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents},
  author={Zhang, Zhuosheng and Yao, Yao and Zhang, Aston and Tang, Xiangru and Ma, Xinbei and He, Zhiwei and Wang, Yiming and Gerstein, Mark and Wang, Rui and Liu, Gongshen and others},
  journal={arXiv preprint arXiv:2311.11797},
  year={2023}
} </pre>
       </div>
    </LI>
</UL>




<UL>
    <LI>
    <B style="color: #224b8d">MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning
</B><BR>
<B>Xiangru Tang*</B>, Anni Zou*, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, Arman Cohan, Mark Gerstein.<BR>
      arXiv, 2023<BR>
      [<a href="https://arxiv.org/abs/2311.10537">PDF</a>]
      [<a href="javascript:toggleBibtex('medagents_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('medagents_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/gersteinlab/MedAgents" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star gersteinlab/MedAgents on GitHub">MedAgents</a>	   
	
	<div id=medagents_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Large Language Models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and the reasoning over specialized knowledge. To address these obstinate issues, we propose a novel Multi-disciplinary Collaboration (MC) framework for the medical domain that leverages role-playing LLM-based agents who participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free and interpretable framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work particularly focuses on the zero-shot scenario, our results on nine data sets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MC framework excels at mining and harnessing the medical expertise in LLMs, as well as extending its reasoning abilities. Based on these outcomes, we further conduct a human evaluation to pinpoint and categorize common errors within our method, as well as ablation studies aimed at understanding the impact of various factors on overall performance.
			    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=medagents_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{tang2023medagents,
  title={MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning},
  author={Tang, Xiangru and Zou, Anni and Zhang, Zhuosheng and Zhao, Yilun and Zhang, Xingyao and Cohan, Arman and Gerstein, Mark},
  journal={arXiv preprint arXiv:2311.10537},
  year={2023}
}</pre>
       </div>
    </LI>
</UL>


<UL>
    <LI>
    <B style="color: #224b8d">ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks
</B><BR>
Yuliang Liu*, <B>Xiangru Tang*</B>, Zefan Cai*, Junjie Lu, Yichi Zhang, Yanjun Shao, Zexuan Deng, Helan Hu, Zengxian Yang, Kaikai An, Ruijun Huang, Shuzheng Si, Sheng Chen, Haozhe Zhao, Zhengliang Li, Liang Chen, Yiming Zong, Yan Wang, Tianyu Liu, Zhiwei Jiang, Baobao Chang, Yujia Qin, Wangchunshu Zhou, Yilun Zhao, Arman Cohan, Mark Gerstein.
<BR>
      arXiv, 2023<BR>
      [<a href="https://arxiv.org/abs/2311.09835">PDF</a>]
      [<a href="javascript:toggleBibtex('ML-Bench_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('ML-Bench_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/gersteinlab/ML-Bench" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star gersteinlab/ML-Bench on GitHub">ML-Bench</a>	   
	
	<div id=ML-Bench_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Large language models have shown promising performance in code generation benchmarks. However, a considerable divide exists between these benchmark achievements and their practical applicability, primarily attributed to real-world programming's reliance on pre-existing libraries. Instead of evaluating LLMs to code from scratch, this work aims to propose a new evaluation setup where LLMs use open-source libraries to finish machine learning tasks. Therefore, we propose ML-Bench, an expansive benchmark developed to assess the effectiveness of LLMs in leveraging existing functions in open-source libraries. Consisting of 10044 samples spanning 130 tasks over 14 notable machine learning GitHub repositories. In this setting, given a specific machine learning task instruction and the accompanying README in a codebase, an LLM is tasked to generate code to accomplish the task. This necessitates the comprehension of long and language-code interleaved documents, as well as the understanding of complex cross-file code structures, introducing new challenges. Notably, while GPT-4 exhibits remarkable improvement over other LLMs, it manages to accomplish only 39.73\% of the tasks, leaving a huge space for improvement. We address these challenges by proposing ML-Agent, designed to effectively navigate the codebase, locate documentation, retrieve code, and generate executable code. Empirical results demonstrate that ML-Agent, built upon GPT-4, results in further improvements.
			    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=ML-Bench_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{liu2023ml,
  title={ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks},
  author={Liu, Yuliang and Tang, Xiangru and Cai, Zefan and Lu, Junjie and Zhang, Yichi and Shao, Yanjun and Deng, Zexuan and Hu, Helan and Yang, Zengxian and An, Kaikai and others},
  journal={arXiv preprint arXiv:2311.09835},
  year={2023}
} </pre>
       </div>
    </LI>
</UL>

<UL>
    <LI>
    <B style="color: #224b8d">Investigating Data Contamination in Modern Benchmarks for Large Language Models

</B><BR>
Chunyuan Deng, Yilun Zhao, <B>Xiangru Tang</B>, Mark Gerstein, Arman Cohan.
<BR>
      arXiv, 2023<BR>
      [<a href="https://arxiv.org/abs/2311.09783">PDF</a>]
      [<a href="javascript:toggleBibtex('InvestigatingData_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('InvestigatingData_bib')" target=_self>Bib</a>]
	
	<div id=InvestigatingData_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Recent observations have underscored a disparity between the inflated benchmark scores and the actual performance of LLMs, raising concerns about the potential contamination of evaluation benchmarks. This issue is especially critical for closed-source models and certain open-source models where training data transparency is lacking. In this paper, we study data contamination by proposing two methods tailored for both open-source and proprietary LLMs. We first introduce a retrieval-based system to explore potential overlaps between evaluation benchmarks and pretraining corpora. We further present a novel investigation protocol named Testset Slot Guessing (TS-Guessing), applicable to both open and proprietary models. This approach entails masking a wrong answer in a multiple-choice question and prompting the model to fill in the gap. Additionally, it involves obscuring an unlikely word in an evaluation example and asking the model to produce it. We find that certain commercial LLMs could surprisingly guess the missing option in various test sets. Specifically, in the TruthfulQA benchmark, we find that LLMs exhibit notable performance improvement when provided with additional metadata in the benchmark. Further, in the MMLU benchmark, ChatGPT and GPT4 demonstrated an exact match rate of 52% and 57%, respectively, in guessing the missing options in benchmark test data. We hope these results underscore the need for more robust evaluation methodologies and benchmarks in the field.			    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=InvestigatingData_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{deng2023investigating,
  title={Investigating Data Contamination in Modern Benchmarks for Large Language Models},
  author={Deng, Chunyuan and Zhao, Yilun and Tang, Xiangru and Gerstein, Mark and Cohan, Arman},
  journal={arXiv preprint arXiv:2311.09783},
  year={2023}
}              </pre>
       </div>
    </LI>
</UL>

<UL>
    <LI>
    <B style="color: #224b8d">Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity

</B><BR>
Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, <B>Xiangru Tang</B>, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang, Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang, Yue Zhang.

<BR>
      arXiv, 2023<BR>
      [<a href="https://arxiv.org/abs/2310.07521">PDF</a>]
      [<a href="javascript:toggleBibtex('Surveyonfactuality_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('Surveyonfactuality_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/wangcunxiang/LLM-Factuality-Survey" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star wangcunxiang/LLM-Factuality-Survey on GitHub">LLM-Factuality-Survey</a>	   
	<div id=Surveyonfactuality_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
This survey addresses the crucial issue of factuality in Large Language Models (LLMs). As LLMs find applications across diverse domains, the reliability and accuracy of their outputs become vital. We define the Factuality Issue as the probability of LLMs to produce content inconsistent with established facts. We first delve into the implications of these inaccuracies, highlighting the potential consequences and challenges posed by factual errors in LLM outputs. Subsequently, we analyze the mechanisms through which LLMs store and process facts, seeking the primary causes of factual errors. Our discussion then transitions to methodologies for evaluating LLM factuality, emphasizing key metrics, benchmarks, and studies. We further explore strategies for enhancing LLM factuality, including approaches tailored for specific domains. We focus on two primary LLM configurations standalone LLMs and Retrieval-Augmented LLMs that utilize external data, we detail their unique challenges and potential enhancements. Our survey offers a structured guide for researchers aiming to fortify the factual reliability of LLMs.
		    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=Surveyonfactuality_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{wang2023survey,
  title={Survey on factuality in large language models: Knowledge, retrieval and domain-specificity},
  author={Wang, Cunxiang and Liu, Xiaoze and Yue, Yuanhao and Tang, Xiangru and Zhang, Tianhang and Jiayang, Cheng and Yao, Yunzhi and Gao, Wenyang and Hu, Xuming and Qi, Zehan and others},
  journal={arXiv preprint arXiv:2310.07521},
  year={2023}
}</pre>
       </div>
    </LI>
</UL>

<UL>
    <LI>
    <B style="color: #224b8d">Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models


</B><BR>

Anni Zou, Zhuosheng Zhang, Hai Zhao, <B>Xiangru Tang</B>.

<BR>
      arXiv, 2023<BR>
      [<a href="https://arxiv.org/abs/2310.06692">PDF</a>]
      [<a href="javascript:toggleBibtex('Meta-CoT_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('Meta-CoT_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/Anni-Zou/Meta-CoT" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star Anni-Zou/Meta-CoT on GitHub">Meta-CoT</a>	   
	<div id=Meta-CoT_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Large language models (LLMs) have unveiled remarkable reasoning capabilities by exploiting chain-of-thought (CoT) prompting, which generates intermediate reasoning chains to serve as the rationale for deriving the answer. However, current CoT methods either simply employ general prompts such as Let's think step by step, or heavily rely on handcrafted task-specific demonstrations to attain preferable performances, thereby engendering an inescapable gap between performance and generalization. To bridge this gap, we propose Meta-CoT, a generalizable CoT prompting method in mixed-task scenarios where the type of input questions is unknown. Meta-CoT firstly categorizes the scenario based on the input question and subsequently constructs diverse demonstrations from the corresponding data pool in an automatic pattern. Meta-CoT simultaneously enjoys remarkable performances on ten public benchmark reasoning tasks and superior generalization capabilities. Notably, Meta-CoT achieves the state-of-the-art result on SVAMP (93.7%) without any additional program-aided methods. Our further experiments on five out-of-distribution datasets verify the stability and generality of Meta-CoT.
		    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=Meta-CoT_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{zou2023metacot,
  title={Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models},
  author={Anni Zou and Zhuosheng Zhang and Hai Zhao and Xiangru Tang},
  journal={arXiv preprint arXiv:2310.06692},
  year={2023}
} </pre>
       </div>
    </LI>
</UL>

<UL>
    <LI>
    <B style="color: #224b8d">Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?


</B><BR>
<B>Xiangru Tang</B>, Yiming Zong, Jason Phang, Yilun Zhao, Wangchunshu Zhou, Arman Cohan, Mark Gerstein.

<BR>
      arXiv, 2023<BR>
      [<a href="https://arxiv.org/abs/2309.08963">PDF</a>]
      [<a href="javascript:toggleBibtex('Struc-Bench_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('Struc-Bench_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/gersteinlab/Struc-Bench" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star gersteinlab/Struc-Bench on GitHub">Struc-Bench</a>	   
	<div id=Struc-Bench_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Despite the power of Large Language Models (LLMs) like GPT-4, they still struggle with tasks that require generating complex, structured outputs. In this study, we assess the capability of Current LLMs in generating complex structured data and propose a structure-aware fine-tuning approach as a solution to improve this ability. To perform a comprehensive evaluation, we propose Struc-Bench, include five representative LLMs (i.e., GPT-NeoX 20B, GPT-3.5, GPT-4, and Vicuna) and evaluate them on our carefully constructed datasets spanning raw text, HTML, and LaTeX tables. Based on our analysis of current model performance, we identify specific common formatting errors and areas of potential improvement. To address complex formatting requirements, we utilize FormatCoT (Chain-of-Thought) to generate format instructions from target outputs. Our experiments show that our structure-aware fine-tuning method, when applied to LLaMA-7B, significantly improves adherence to natural language constraints, outperforming other evaluated LLMs. Based on these results, we present an ability map of model capabilities from six dimensions (i.e., coverage, formatting, reasoning, comprehension, pragmatics, and hallucination). This map highlights the weaknesses of LLMs in handling complex structured outputs and suggests promising directions for future work. Our code and models can be found at https://github.com/gersteinlab/Struc-Bench.
		    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=Struc-Bench_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{tang2023struc,
  title={Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?},
  author={Tang, Xiangru and Zong, Yiming and Zhao, Yilun and Cohan, Arman and Gerstein, Mark},
  journal={arXiv preprint arXiv:2309.08963},
  year={2023}
} </pre>
       </div>
    </LI>
</UL>
<UL>
    <LI>
    <B style="color: #224b8d">BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge


</B><BR>

<B>Xiangru Tang*</B>, Bill Qian*, Rick Gao, Jiakang Chen, Xinyun Chen, Mark Gerstein.

<BR>
      arXiv, 2023<BR>
      [<a href="https://arxiv.org/abs/2308.16458">PDF</a>]
      [<a href="javascript:toggleBibtex('BioCoder_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('BioCoder_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/gersteinlab/biocoder" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star gersteinlab/biocoder on GitHub">BioCoder</a>	   
	<div id=BioCoder_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+, and ChatGPT. Our detailed analysis of these models emphasizes the importance of domain knowledge, pragmatic code generation, and contextual understanding. Our dataset, benchmark, Docker images, and scripts required for testing are all available at https://github.com/gersteinlab/biocoder.
		    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=BioCoder_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{tang2023biocoder,
  title={BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge},
  author={Tang, Xiangru and Qian, Bill and Gao, Rick and Chen, Jiakang and Chen, Xinyun and Gerstein, Mark},
  journal={arXiv preprint arXiv:2308.16458},
  year={2023}
} </pre>
       </div>
    </LI>
</UL>
<UL>
    <LI>
    <B style="color: #224b8d">OctoPack: Instruction Tuning Code Large Language Models




</B><BR>

Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, <B>Xiangru Tang</B>, Leandro von Werra, Shayne Longpre.

<BR>
      arXiv, 2023<BR>
      [<a href="https://arxiv.org/abs/2308.07124">PDF</a>]
      [<a href="javascript:toggleBibtex('OctoPack_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('OctoPack_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/bigcode-project/octopack" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star bigcode-project/octopack on GitHub">Octopack</a>	   
	<div id=OctoPack_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.
		    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=OctoPack_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{muennighoff2023octopack,
      title={OctoPack: Instruction Tuning Code Large Language Models}, 
      author={Niklas Muennighoff and Qian Liu and Armel Zebaze and Qinkai Zheng and Binyuan Hui and Terry Yue Zhuo and Swayam Singh and Xiangru Tang and Leandro von Werra and Shayne Longpre},
      journal={arXiv preprint arXiv:2308.07124},
      year={2023}
}   </pre>
       </div>
    </LI>
</UL>
<UL>
    <LI>
    <B style="color: #224b8d"> ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs



</B><BR>

Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, <B>Xiangru Tang</B>, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, Maosong Sun.

<BR>
      arXiv, 2023<BR>
      [<a href="https://arxiv.org/abs/2307.16789">PDF</a>]
      [<a href="javascript:toggleBibtex('ToolLLM_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('ToolLLM_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/OpenBMB/ToolBench" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star OpenBMB/ToolBench on GitHub">ToolLLM</a>	   
	<div id=ToolLLM_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.
		    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=ToolLLM_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{qin2023toolllm,
  title={Toolllm: Facilitating large language models to master 16000+ real-world apis},
  author={Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and others},
  journal={arXiv preprint arXiv:2307.16789},
  year={2023}
} </pre>
       </div>
    </LI>
</UL>

<UL>
    <LI>
    <B style="color: #224b8d">


Aligning factual consistency for clinical studies summarization through reinforcement learning



</B><BR>

<B>Xiangru Tang</B>, Arman Cohan, Mark Gerstein.


<BR>
      ACL ClinicalNLP Workshop, 2023<BR>
      [<a href="https://aclanthology.org/2023.clinicalnlp-1.7/">PDF</a>]
      [<a href="javascript:toggleBibtex('AligningFactual_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('AligningFactual_bib')" target=_self>Bib</a>]
	<div id=AligningFactual_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
In the rapidly evolving landscape of medical research, accurate and concise summarization of clinical studies is crucial to support evidence-based practice. This paper presents a novel approach to clinical studies summarization, leveraging reinforcement learning to enhance factual consistency and align with human annotator preferences. Our work focuses on two tasks: Conclusion Generation and Review Generation. We train a CONFIT summarization model that outperforms GPT-3 and previous state-of-the-art models on the same datasets and collects expert and crowd-worker annotations to evaluate the quality and factual consistency of the generated summaries. These annotations enable us to measure the correlation of various automatic metrics, including modern factual evaluation metrics like QAFactEval, with human-assessed factual consistency. By employing top-correlated metrics as objectives for a reinforcement learning model, we demonstrate improved factuality in generated summaries that are preferred by human annotators.
		    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=AligningFactual_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{tang-etal-2023-aligning,
    title = "Aligning Factual Consistency for Clinical Studies Summarization through Reinforcement Learning",
    author = "Tang, Xiangru  and
      Cohan, Arman  and
      Gerstein, Mark",
    editor = "Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna",
    booktitle = "Proceedings of the 5th Clinical Natural Language Processing Workshop",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.clinicalnlp-1.7",
    doi = "10.18653/v1/2023.clinicalnlp-1.7",
    pages = "48--58",
}</pre>
       </div>
    </LI>
</UL>

<UL>
    <LI>
    <B style="color: #224b8d">


GersteinLab at MEDIQA-Chat 2023: Clinical Note Summarization from Doctor-Patient Conversations through Fine-tuning and In-context Learning

</B><BR>

<B>Xiangru Tang</B>, Andrew Tran, Jeffrey Tan, Mark Gerstein.

<BR>
      ACL ClinicalNLP Workshop, 2023<BR>
      [<a href="https://aclanthology.org/2023.clinicalnlp-1.58">PDF</a>]
      [<a href="javascript:toggleBibtex('MEDIQA-Chat2023_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('MEDIQA-Chat2023_bib')" target=_self>Bib</a>]
	<div id=MEDIQA-Chat2023_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
This paper presents our contribution to the MEDIQA-2023 Dialogue2Note shared task, encompassing both subtask A and subtask B. We approach the task as a dialogue summarization problem and implement two distinct pipelines: (a) a fine-tuning of a pre-trained dialogue summarization model and GPT-3, and (b) few-shot in-context learning (ICL) using a large language model, GPT-4. Both methods achieve excellent results in terms of ROUGE-1 F1, BERTScore F1 (deberta-xlarge-mnli), and BLEURT, with scores of 0.4011, 0.7058, and 0.5421, respectively. Additionally, we predict the associated section headers using RoBERTa and SciBERT based classification models. Our team ranked fourth among all teams, while each team is allowed to submit three runs as part of their submission. We also utilize expert annotations to demonstrate that the notes generated through the ICL GPT-4 are better than all other baselines. The code for our submission is available.
		    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=MEDIQA-Chat2023_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{tang-etal-2023-gersteinlab,
    title = "{G}erstein{L}ab at {MEDIQA}-Chat 2023: Clinical Note Summarization from Doctor-Patient Conversations through Fine-tuning and In-context Learning",
    author = "Tang, Xiangru  and
      Tran, Andrew  and
      Tan, Jeffrey  and
      Gerstein, Mark",
    editor = "Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna",
    booktitle = "Proceedings of the 5th Clinical Natural Language Processing Workshop",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.clinicalnlp-1.58",
    doi = "10.18653/v1/2023.clinicalnlp-1.58",
    pages = "546--554",
}          </pre>
       </div>
    </LI>
</UL>
<UL>
    <LI>
    <B style="color: #224b8d">

QTSumm: Query-Focused Summarization over Tabular Data

</B><BR>

Yilun Zhao, Zhenting Qi, Linyong Nan, Boyu Mi, Yixin Liu, Weijin Zou, Simeng Han, Ruizhe Chen, <B>Xiangru Tang</B>, Yumo Xu, Dragomir Radev, Arman Cohan.

<BR>
      EMNLP, 2023<BR>
      [<a href="https://arxiv.org/abs/2305.14303">PDF</a>]
      [<a href="javascript:toggleBibtex('QTSumm_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('QTSumm_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/yale-nlp/QTSumm" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star yale-nlp/QTSumm on GitHub">QTSumm</a>	   
	<div id=QTSumm_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
People primarily consult tables to conduct data analysis or answer specific questions. Text generation systems that can provide accurate table summaries tailored to users' information needs can facilitate more efficient access to relevant data insights. Motivated by this, we define a new query-focused table summarization task, where text generation models have to perform human-like reasoning and analysis over the given table to generate a tailored summary. We introduce a new benchmark named QTSumm for this task, which contains 7,111 human-annotated query-summary pairs over 2,934 tables covering diverse topics. We investigate a set of strong baselines on QTSumm, including text generation, table-to-text generation, and large language models. Experimental results and manual analysis reveal that the new task presents significant challenges in table-to-text generation for future research. Moreover, we propose a new approach named ReFactor, to retrieve and reason over query-relevant information from tabular data to generate several natural language facts. Experimental results demonstrate that ReFactor can bring improvements to baselines by concatenating the generated facts to the model input.		    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=QTSumm_bib class=blockcontent style="DISPLAY: none">
            <pre>
@misc{zhao2023qtsumm,
      title={QTSUMM: Query-Focused Summarization over Tabular Data}, 
      author={Yilun Zhao and Zhenting Qi and Linyong Nan and Boyu Mi and Yixin Liu and Weijin Zou and Simeng Han and Xiangru Tang and Yumo Xu and Arman Cohan and Dragomir Radev},
      year={2023},
      eprint={2305.14303},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}         </pre>
       </div>
    </LI>
</UL>
<UL>
    <LI>
    <B style="color: #224b8d">

Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios

</B><BR>

Yilun Zhao*, Haowei Zhang*, Shengyun Si*, Linyong Nan, <B>Xiangru Tang</B>, Arman Cohan.


<BR>
      EMNLP, 2023<BR>
      [<a href="https://arxiv.org/abs/2305.14987">PDF</a>]
      [<a href="javascript:toggleBibtex('LLM-T2T_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('LLM-T2T_bib')" target=_self>Bib</a>]
<div style="padding-top:5px"> <a class="github-button" href="https://github.com/yale-nlp/LLM-T2T" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star yale-nlp/LLM-T2T on GitHub">LLM-T2T</a>	   
	<div id=LLM-T2T_abs class=blockcontent style="DISPLAY: none">
	    <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Tabular data is prevalent across various industries, necessitating significant time and effort for users to understand and manipulate for their information-seeking purposes. The advancements in large language models (LLMs) have shown enormous potential to improve user efficiency. However, the adoption of LLMs in real-world applications for table information seeking remains underexplored. In this paper, we investigate the table-to-text capabilities of different LLMs using four datasets within two real-world information seeking scenarios. These include the LogicNLG and our newly-constructed LoTNLG datasets for data insight generation, along with the FeTaQA and our newly-constructed F2WTQ datasets for query-based generation. We structure our investigation around three research questions, evaluating the performance of LLMs in table-to-text generation, automated evaluation, and feedback generation, respectively. Experimental results indicate that the current high-performing LLM, specifically GPT-4, can effectively serve as a table-to-text generator, evaluator, and feedback generator, facilitating users' information seeking purposes in real-world scenarios. However, a significant performance gap still exists between other open-sourced LLMs (e.g., Tulu and LLaMA-2) and GPT-4 models. 	    </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=LLM-T2T_bib class=blockcontent style="DISPLAY: none">
            <pre>
@misc{zhao2023investigating,
      title={Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios}, 
      author={Yilun Zhao and Haowei Zhang and Shengyun Si and Linyong Nan and Xiangru Tang and Arman Cohan},
      year={2023},
      eprint={2305.14987},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}     </pre>
       </div>
    </LI>
</UL>

































		  




		  

 <br>

           </div>
      </div>
   </article>
  </div>	  	  




	      
		      		  
  
<div class="post">
  <article>
    <div class="cv">
          <h2 id="talks"><font size="5" color="#0f4d92"><b>Recent Talks</b></font></h2>
          <div>
 <strong>01/2024 </strong> Talk at Pacific Symposium on Biocomputing (PSB) 2024 Workshop on LLMs and ChatGPT for Biomedicine.

 <br>
 <strong>07/2023 </strong> Talk at ISMB/ECCB 2023 Text Mining Section.


		   <br>
 <br>

           </div>
      </div>
   </article>
  </div>	  	  


	      
  
<div class="post">
  <article>
    <div class="cv">
          <h2 id="services"><font size="5" color="#0f4d92"><b>Professional Services</b></font></h2>
          <div>

 		  <strong>Orangzer:</strong> SIGDIAL/INLG 2023 Workshop on Taming Large Language Models.
 <br>
 <strong>Conference Program Committee / Reviewer</strong>: NeurIPS, ICML, ACL, EMNLP, CIKM, NAACL, INLG.
<br>
 
 <strong>Journal Reviewer</strong>: Neurocomputing, PLOS ONE, Health Data Science.

		   <br>
 <br>

           </div>
      </div>
   </article>
  </div>	  	  

	  

<div class="post">
  <article>
    <div class="cv">
          <h2 id="teaching"><font size="5" color="#0f4d92"><b>Teaching</b></font></h2>
          <div>

<strong>Teaching Fellow </strong> for CPSC 452 / CPSC 552 / AMTH 552 / CB&B 663 Deep Learning Theory and Applications, <i>Yale University</i>, 2023.
 <br> 
<strong>Teaching Fellow </strong> for CPSC 437 / CPSC 537 Introduction to Database Systems, <i>Yale University</i>, 2023.

		   <br>
		   <br>
          </div>
      </div>
  </article>
</div>



<div class="post">
  <article>
    <div class="cv">
          <h2 id="misc"><font size="5" color="#0f4d92"><b>Misc.</b></font></h2>
          <div>

<strong>My 12 coursework at Yale:</strong> CPSC 523 Principles of Operating Systems, 537 Intro to Database, 539 Software Engineering, 552 Deep Learning Theory, 553 Unsupervised Learning, 569 Randomized Algorithms, 577 NLP, 583 Deep Learning on Graph, 668 Blockchain Research, 677 Adv NLP, 680 Trustworthy Deep Learning, 752 Biomedical Data Sci.

		   <br>
          </div>
      </div>
  </article>
</div>

	      
	      
	      
	      <!-- 进度条-->

<script type="text/javascript">
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>


	      
	      
	      
	      
<!-- Default Statcounter code for Homepage https://xiangrutang.github.io/ -->
<script type="text/javascript">
var sc_project=12795012; 
var sc_invisible=0; 
var sc_security="88264835"; 
var scJsHost = "https://";
document.write("<sc"+"ript type='text/javascript' src='" + scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img class="statcounter"
src="https://c.statcounter.com/12795012/0/88264835/0/" alt="Web Analytics"
referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
<!-- End of Statcounter Code -->

</body>
</html>

<!-- Localized -->
